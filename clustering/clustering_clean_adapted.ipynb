{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "ratings_df_schema = StructType(\n",
    "    [StructField('asin', StringType()),\n",
    "    StructField('helpful', ArrayType(IntegerType())),\n",
    "    StructField('overall', FloatType()),\n",
    "    StructField('reviewText', StringType()),\n",
    "    StructField('reviewTime', DateType()),\n",
    "    StructField('reviewerID', StringType()),\n",
    "    StructField('summary', StringType())])\n",
    "\n",
    "# StructField('unixReviewTime', LongType())\n",
    "# ['asin', 'description', 'title', 'categories']\n",
    "metadata_df_schema = StructType(\n",
    "    [StructField('asin', StringType()),\n",
    "    StructField('description', StringType()),\n",
    "    StructField('title', StringType()),\n",
    "    StructField('categories', ArrayType(StringType()))])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#import metadata files\n",
    "\n",
    "import gzip\n",
    "from pyspark.sql import Row\n",
    "from collections import OrderedDict\n",
    "from datetime import datetime\n",
    "\n",
    "def parse_metadata(path):\n",
    "    g = gzip.open(path, 'rb')\n",
    "    for l in g:\n",
    "        yield eval(l)\n",
    "\n",
    "def create_metadata_df(path):\n",
    "    metadata = []\n",
    "    for entry in parse_metadata(path):\n",
    "        metadata.append(entry)\n",
    "\n",
    "  #find all keys\n",
    "    all_keys = []\n",
    "    for i in range(len(metadata)):\n",
    "        for key in metadata[i].keys():\n",
    "            if key not in all_keys:\n",
    "                all_keys.append(key)\n",
    "    print(all_keys)\n",
    "  #['asin', 'categories', 'price', 'related'] in instant videos\n",
    "  #['asin', 'imUrl', 'description', 'categories', 'title', 'related', 'price', 'salesRank', 'brand'] in electronics\n",
    "  \n",
    "  #check number of entries that contains each key\n",
    "    key_count_dict = {}\n",
    "    for key in all_keys:\n",
    "        key_count_dict[key] = 0\n",
    "    for i in range(len(metadata)):\n",
    "        for key in all_keys:\n",
    "            if key in metadata[i].keys():\n",
    "                key_count_dict[key] += 1\n",
    "    print(key_count_dict)  \n",
    "#  {'asin': 498196, 'description': 459470, 'title': 491194, 'price': 389693, 'imUrl': 498021, 'related': 366959, 'salesRank': 128706, 'brand': 142532, 'categories': 498196} for electronics\n",
    "    discard_missing_info_col = True\n",
    "    cols_to_keep = ['asin', 'description', 'title', 'categories']\n",
    "    filtered_metadata = []\n",
    "    for i in range(len(metadata)):\n",
    "        keep_col = True\n",
    "        if discard_missing_info_col:\n",
    "      #only retain items that contain info for all columns\n",
    "            cols = [x for x in cols_to_keep if x in metadata[i].keys()]\n",
    "            if len(cols) != len(cols_to_keep):\n",
    "                keep_col = False\n",
    "        if keep_col:\n",
    "            temp_dict = {}\n",
    "            for col in cols_to_keep: \n",
    "                if col not in metadata[i].keys():\n",
    "                    temp_dict[col] = None\n",
    "                elif col == 'categories':#given as array of array so only keeep the inner array\n",
    "                    temp_dict[col] = metadata[i]['categories'][0]\n",
    "                else:\n",
    "                    temp_dict[col] = metadata[i][col]\n",
    "            filtered_metadata.append(temp_dict)\n",
    "\n",
    "    return sqlContext.createDataFrame(filtered_metadata, schema = metadata_df_schema)#schema=metadata_df_schema\n",
    "  \n",
    "\n",
    "# books_df = create_review_df('/dbfs/tmp/Metadata/Books.json.gz')\n",
    "#instant_video_meta_df = create_metadata_df('/dbfs/tmp/Metadata/Instant_Video.json.gz')\n",
    "# movies_df = create_review_df('/dbfs/tmp/Metadata/Movies_and_TV.json.gz')\n",
    "# cds_df = create_review_df('/dbfs/tmp/Metadata/CDs_and_Vinyl.json.gz')\n",
    "# instruments_df = create_review_df('/dbfs/tmp/Metadata/Musical_Instruments.json.gz')\n",
    "# instant_video_df = create_review_df('/dbfs/tmp/Metadata/Instant_Videos.json.gz'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['asin', 'categories', 'description', 'title', 'price', 'imUrl', 'brand', 'related', 'salesRank']\n",
      "{'asin': 71317, 'categories': 71317, 'description': 65642, 'title': 71241, 'price': 57741, 'imUrl': 71243, 'brand': 27858, 'related': 58721, 'salesRank': 36}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[asin: string, description: string, title: string, categories: array<string>]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baby_meta_df = create_metadata_df('spark_notebooks/meta_Baby.json.gz')\n",
    "baby_meta_df.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+--------------------+----------+\n",
      "|      asin|         description|               title|categories|\n",
      "+----------+--------------------+--------------------+----------+\n",
      "|0188399313|Wee-Go Glass baby...|Lifefactory 4oz B...|    [Baby]|\n",
      "|0188399518|The Planet Wise F...|Planetwise Flanne...|    [Baby]|\n",
      "|0188399399|The Planet Wise W...|Planetwise Wipe P...|    [Baby]|\n",
      "+----------+--------------------+--------------------+----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "baby_meta_df.show(3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df1 = pd.read_csv('spark_notebooks/baby_product_summary.csv')[['asin','rating_count']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rating_count_df = sqlContext.createDataFrame(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "baby_meta_rc_df = baby_meta_df.join(rating_count_df, baby_meta_df['asin']==rating_count_df['asin'], 'inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filtered_baby_meta_rc_df = baby_meta_rc_df.filter(baby_meta_rc_df.rating_count>=5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+--------------------+----------+----------+------------+\n",
      "|      asin|         description|               title|categories|      asin|rating_count|\n",
      "+----------+--------------------+--------------------+----------+----------+------------+\n",
      "|0615447279|Thumbuddy To Love...|Stop Pacifier Suc...|    [Baby]|0615447279|         9.0|\n",
      "|097293751X|Easily keep track...|Baby Tracker&reg;...|    [Baby]|097293751X|        42.0|\n",
      "|0980027500|This calendar pro...|Nature's Lullabie...|    [Baby]|0980027500|        12.0|\n",
      "|0980027586|This extra sticke...|Nature's Lullabie...|    [Baby]|0980027586|         8.0|\n",
      "|0980027594|This calendar pro...|Nature's Lullabie...|    [Baby]|0980027594|        31.0|\n",
      "+----------+--------------------+--------------------+----------+----------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filtered_baby_meta_rc_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2968\n"
     ]
    }
   ],
   "source": [
    "def find_brands(path):\n",
    "    brands = []\n",
    "    count = 0\n",
    "    for entry in parse_metadata(path):\n",
    "        if 'brand' in entry.keys() and entry['brand'] not in brands:\n",
    "            brands.append(entry['brand'])\n",
    "    brands.pop(brands.index(\"\"))\n",
    "    return brands\n",
    "   \n",
    "brands_stop_words = find_brands('spark_notebooks/meta_Baby.json.gz')\n",
    "print(len(brands_stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o61.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 3.0 failed 1 times, most recent failure: Lost task 2.0 in stage 3.0 (TID 8, localhost, executor driver): java.io.FileNotFoundException: /tmp/blockmgr-953fc243-7dca-4840-ae2a-1e095a2cc49d/3c/temp_shuffle_eee0b933-c258-4980-8ef4-23ab571e8fb5 (Too many open files)\n\tat java.io.FileOutputStream.open(FileOutputStream.java:286)\n\tat java.io.FileOutputStream.<init>(FileOutputStream.java:226)\n\tat org.apache.spark.storage.DiskBlockObjectWriter.initialize(DiskBlockObjectWriter.scala:102)\n\tat org.apache.spark.storage.DiskBlockObjectWriter.open(DiskBlockObjectWriter.scala:115)\n\tat org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:235)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:152)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1153)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.lang.Thread.run(Thread.java:785)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat java.lang.Thread.getStackTrace(Thread.java:1117)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1925)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1938)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1951)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:333)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\n\tat org.apache.spark.sql.Dataset$$anonfun$org$apache$spark$sql$Dataset$$execute$1$1.apply(Dataset.scala:2386)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:57)\n\tat org.apache.spark.sql.Dataset.withNewExecutionId(Dataset.scala:2788)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$execute$1(Dataset.scala:2385)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collect(Dataset.scala:2392)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2128)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2127)\n\tat org.apache.spark.sql.Dataset.withTypedCallback(Dataset.scala:2818)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2127)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2342)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:248)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:95)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:55)\n\tat java.lang.reflect.Method.invoke(Method.java:508)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:785)\nCaused by: java.io.FileNotFoundException: /tmp/blockmgr-953fc243-7dca-4840-ae2a-1e095a2cc49d/3c/temp_shuffle_eee0b933-c258-4980-8ef4-23ab571e8fb5 (Too many open files)\n\tat java.io.FileOutputStream.open(FileOutputStream.java:286)\n\tat java.io.FileOutputStream.<init>(FileOutputStream.java:226)\n\tat org.apache.spark.storage.DiskBlockObjectWriter.initialize(DiskBlockObjectWriter.scala:102)\n\tat org.apache.spark.storage.DiskBlockObjectWriter.open(DiskBlockObjectWriter.scala:115)\n\tat org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:235)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:152)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1153)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-a8a87a72062d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfiltered_baby_meta_rc_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/spark-2.1.1-bin-hadoop2.7/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate)\u001b[0m\n\u001b[1;32m    316\u001b[0m         \"\"\"\n\u001b[1;32m    317\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 318\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    319\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark-2.1.1-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    317\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    318\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    320\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o61.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 3.0 failed 1 times, most recent failure: Lost task 2.0 in stage 3.0 (TID 8, localhost, executor driver): java.io.FileNotFoundException: /tmp/blockmgr-953fc243-7dca-4840-ae2a-1e095a2cc49d/3c/temp_shuffle_eee0b933-c258-4980-8ef4-23ab571e8fb5 (Too many open files)\n\tat java.io.FileOutputStream.open(FileOutputStream.java:286)\n\tat java.io.FileOutputStream.<init>(FileOutputStream.java:226)\n\tat org.apache.spark.storage.DiskBlockObjectWriter.initialize(DiskBlockObjectWriter.scala:102)\n\tat org.apache.spark.storage.DiskBlockObjectWriter.open(DiskBlockObjectWriter.scala:115)\n\tat org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:235)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:152)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1153)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.lang.Thread.run(Thread.java:785)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat java.lang.Thread.getStackTrace(Thread.java:1117)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1925)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1938)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1951)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:333)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\n\tat org.apache.spark.sql.Dataset$$anonfun$org$apache$spark$sql$Dataset$$execute$1$1.apply(Dataset.scala:2386)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:57)\n\tat org.apache.spark.sql.Dataset.withNewExecutionId(Dataset.scala:2788)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$execute$1(Dataset.scala:2385)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collect(Dataset.scala:2392)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2128)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2127)\n\tat org.apache.spark.sql.Dataset.withTypedCallback(Dataset.scala:2818)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2127)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2342)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:248)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:95)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:55)\n\tat java.lang.reflect.Method.invoke(Method.java:508)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:785)\nCaused by: java.io.FileNotFoundException: /tmp/blockmgr-953fc243-7dca-4840-ae2a-1e095a2cc49d/3c/temp_shuffle_eee0b933-c258-4980-8ef4-23ab571e8fb5 (Too many open files)\n\tat java.io.FileOutputStream.open(FileOutputStream.java:286)\n\tat java.io.FileOutputStream.<init>(FileOutputStream.java:226)\n\tat org.apache.spark.storage.DiskBlockObjectWriter.initialize(DiskBlockObjectWriter.scala:102)\n\tat org.apache.spark.storage.DiskBlockObjectWriter.open(DiskBlockObjectWriter.scala:115)\n\tat org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:235)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:152)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1153)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "filtered_baby_meta_rc_df.show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "#print # of distinct categories among all products\n",
    "print(baby_meta_df.select('categories').distinct().count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "119\n",
      "['a', 'able', 'about', 'across', 'after', 'all', 'almost', 'also', 'am', 'among', 'an', 'and', 'any', 'are', 'as', 'at', 'be', 'because', 'been', 'but', 'by', 'can', 'cannot', 'could', 'dear', 'did', 'do', 'does', 'either', 'else', 'ever', 'every', 'for', 'from', 'get', 'got', 'had', 'has', 'have', 'he', 'her', 'hers', 'him', 'his', 'how', 'however', 'i', 'if', 'in', 'into', 'is', 'it', 'its', 'just', 'least', 'let', 'like', 'likely', 'may', 'me', 'might', 'most', 'must', 'my', 'neither', 'no', 'nor', 'not', 'of', 'off', 'often', 'on', 'only', 'or', 'other', 'our', 'own', 'rather', 'said', 'say', 'says', 'she', 'should', 'since', 'so', 'some', 'than', 'that', 'the', 'their', 'them', 'then', 'there', 'these', 'they', 'this', 'tis', 'to', 'too', 'twas', 'us', 'wants', 'was', 'we', 'were', 'what', 'when', 'where', 'which', 'while', 'who', 'whom', 'why', 'will', 'with', 'would', 'yet', 'you', 'your']\n"
     ]
    }
   ],
   "source": [
    "#import stopwords\n",
    "stopwords_path = 'spark_notebooks/stopwords_eng.txt'\n",
    "stopwords_rdd = sc.textFile(stopwords_path)\n",
    "stopwords = stopwords_rdd.collect()\n",
    "print(len(stopwords))\n",
    "print(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "\"Reference 'asin' is ambiguous, could be: asin#0, asin#55.;\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m~/spark-2.1.1-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    318\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    320\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o61.apply.\n: org.apache.spark.sql.AnalysisException: Reference 'asin' is ambiguous, could be: asin#0, asin#55.;\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolve(LogicalPlan.scala:264)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveQuoted(LogicalPlan.scala:168)\n\tat org.apache.spark.sql.Dataset.resolve(Dataset.scala:217)\n\tat org.apache.spark.sql.Dataset.col(Dataset.scala:1083)\n\tat org.apache.spark.sql.Dataset.apply(Dataset.scala:1069)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:95)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:55)\n\tat java.lang.reflect.Method.invoke(Method.java:508)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:785)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-e2c274835687>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m parse_row_udf = udf(parse_row, ArrayType(StructType([StructField('_1', LongType()),\n\u001b[1;32m     32\u001b[0m                                                          StructField('_2', StringType())])))\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0mtuple_baby_meta_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbaby_meta_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_baby_meta_rc_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masin\u001b[0m\u001b[0;34m,\u001b[0m                                                        \u001b[0mparse_row_udf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_baby_meta_rc_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdescription\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malias\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'description_features'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m                                                        \u001b[0mparse_row_udf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_baby_meta_rc_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malias\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'title_features'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m                                                        \u001b[0mparse_row_udf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_baby_meta_rc_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcategories\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malias\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'category_features'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mbaby_meta_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_cached\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark-2.1.1-bin-hadoop2.7/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    972\u001b[0m             raise AttributeError(\n\u001b[1;32m    973\u001b[0m                 \"'%s' object has no attribute '%s'\" % (self.__class__.__name__, name))\n\u001b[0;32m--> 974\u001b[0;31m         \u001b[0mjc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    975\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    976\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark-2.1.1-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: \"Reference 'asin' is ambiguous, could be: asin#0, asin#55.;\""
     ]
    }
   ],
   "source": [
    "#parse each column by itself instead\n",
    "# feature hashing\n",
    "#convert each row of metadata_df to list of tuple format \n",
    "from pyspark.sql.functions import udf, array, split\n",
    "import re\n",
    "\n",
    "def parse_row(col):\n",
    "    if col == None:\n",
    "        return [(0, 'EmptyString')]\n",
    "    elif isinstance(col, str):\n",
    "        #remove brands first\n",
    "        for brand in brands_stop_words:\n",
    "            if brand in col:\n",
    "                col = col.replace(brand, \"\")\n",
    "                break\n",
    "        \n",
    "        words = re.split(r'\\W+',col)\n",
    "        \n",
    "    elif type(col) is list:\n",
    "        words = col\n",
    "   \n",
    "  #assign featureID to its index in all_words (e.g. des_words_ID = 0, title_words_ID = 1, etc.)\n",
    "    tuple_list = []\n",
    "    for word in words:\n",
    "        lower_case_word = word.lower()\n",
    "        if lower_case_word not in stopwords:\n",
    "            tuple_list.append((0, lower_case_word))\n",
    "    return tuple_list\n",
    "\n",
    "  \n",
    "parse_row_udf = udf(parse_row, ArrayType(StructType([StructField('_1', LongType()),\n",
    "                                                         StructField('_2', StringType())])))\n",
    "tuple_baby_meta_df = baby_meta_df.select(filtered_baby_meta_rc_df.asin, \\\n",
    "                                                       parse_row_udf(filtered_baby_meta_rc_df.description).alias('description_features'), \\\n",
    "                                                       parse_row_udf(filtered_baby_meta_rc_df.title).alias('title_features'), \\\n",
    "                                                       parse_row_udf(filtered_baby_meta_rc_df.categories).alias('category_features'))\n",
    "                                                                                                                    \n",
    "if baby_meta_df.is_cached:\n",
    "    baby_meta_df.unpersist()\n",
    "if not tuple_baby_meta_df.is_cached:\n",
    "    tuple_baby_meta_df.cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+--------------------+-----------------+\n",
      "|      asin|description_features|      title_features|category_features|\n",
      "+----------+--------------------+--------------------+-----------------+\n",
      "|0188399313|[[0,wee], [0,go],...|[[0,], [0,4oz], [...|       [[0,baby]]|\n",
      "|0188399518|[[0,flannel], [0,...|[[0,planetwise], ...|       [[0,baby]]|\n",
      "|0188399399|[[0,wipe], [0,pou...|[[0,planetwise], ...|       [[0,baby]]|\n",
      "+----------+--------------------+--------------------+-----------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tuple_baby_meta_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "#check for null elements for each feature\n",
    "print(tuple_baby_meta_df.where(tuple_baby_meta_df.description_features.isNull()).count())\n",
    "print(tuple_baby_meta_df.where(tuple_baby_meta_df.title_features.isNull()).count())\n",
    "print(tuple_baby_meta_df.where(tuple_baby_meta_df.category_features.isNull()).count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reduce(func, seq, initial):\n",
    "    fix = initial\n",
    "    for item in seq:\n",
    "        fix = func(fix,item)\n",
    "    return fix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import hashlib\n",
    "\n",
    "def hash_function(raw_feats, num_buckets, print_mapping=False):\n",
    "    \"\"\"Calculate a feature dictionary for an observation's features based on hashing.\n",
    "\n",
    "    Note:\n",
    "        Use print_mapping=True for debug purposes and to better understand how the hashing works.\n",
    "\n",
    "    Args:\n",
    "        raw_feats (list of (int, str)): A list of features for an observation.  Represented as\n",
    "            (featureID, value) tuples.\n",
    "        num_buckets (int): Number of buckets to use as features.\n",
    "        print_mapping (bool, optional): If true, the mappings of featureString to index will be\n",
    "            printed.\n",
    "\n",
    "    Returns:\n",
    "        dict of int to float:  The keys will be integers which represent the buckets that the\n",
    "            features have been hashed to.  The value for a given key will contain the count of the\n",
    "            (featureID, value) tuples that have hashed to that key.\n",
    "    \"\"\"\n",
    "    mapping = { category + ':' + str(ind):\n",
    "                int(int(hashlib.md5((category + ':' + str(ind)).encode('utf-8')).hexdigest(), 16) % num_buckets)\n",
    "                for ind, category in raw_feats}\n",
    "    if(print_mapping): print(mapping)\n",
    "\n",
    "    def map_update(l, r):\n",
    "        l[r] += 1.0\n",
    "        return l\n",
    "\n",
    "    sparse_features = reduce(map_update, mapping.values(), defaultdict(float))\n",
    "    return dict(sparse_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------------+--------------------+----------------------+\n",
      "|      asin|description_hash_features| title_hash_features|category_hash_features|\n",
      "+----------+-------------------------+--------------------+----------------------+\n",
      "|0188399313|     (128,[0,1,5,12,16...|(32,[11,12,13,15,...|         (1,[0],[1.0])|\n",
      "|0188399518|     (128,[6,26,30,32,...|(32,[0,5,18],[1.0...|         (1,[0],[1.0])|\n",
      "|0188399399|     (128,[1,5,6,11,17...|(32,[0,21,30],[1....|         (1,[0],[1.0])|\n",
      "+----------+-------------------------+--------------------+----------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "num_hash_buckets_description = 2 ** 6\n",
    "num_hash_buckets_title = 2 ** 5\n",
    "num_hash_buckets_category = 2 ** 0\n",
    "\n",
    "# UDF that returns a vector of hashed features given an Array of tuples\n",
    "\n",
    "def tuples_to_hash_features_desc(col):\n",
    "  return Vectors.sparse(num_hash_buckets_description, hash_function(col, num_hash_buckets_description))\n",
    "tuples_to_hash_features_desc_udf = udf(tuples_to_hash_features_desc, VectorUDT())\n",
    "\n",
    "def tuples_to_hash_features_title(col):\n",
    "  return Vectors.sparse(num_hash_buckets_title, hash_function(col, num_hash_buckets_title))\n",
    "tuples_to_hash_features_title_udf = udf(tuples_to_hash_features_title, VectorUDT())\n",
    "\n",
    "def tuples_to_hash_features_category(col):\n",
    "  return Vectors.sparse(num_hash_buckets_category, hash_function(col, num_hash_buckets_category))\n",
    "tuples_to_hash_features_category_udf = udf(tuples_to_hash_features_category, VectorUDT())\n",
    "\n",
    "\n",
    "def add_hashed_features(df):\n",
    "    \"\"\"Return a DataFrame with labels and hashed features.\n",
    "\n",
    "    Note:\n",
    "        Make sure to cache the DataFrame that you are returning.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame with 'tuples' column): A DataFrame containing the tuples to be hashed.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: A DataFrame with a 'label' column and a 'features' column that contains a\n",
    "            SparseVector of hashed features.\n",
    "    \"\"\"\n",
    "    return (df.select(df.asin, \\\n",
    "                      tuples_to_hash_features_desc_udf(df.description_features).alias('description_hash_features'), \\\n",
    "                      tuples_to_hash_features_title_udf(df.title_features).alias('title_hash_features'), \\\n",
    "                      tuples_to_hash_features_category_udf(df.category_features).alias('category_hash_features')))\n",
    "\n",
    "hash_baby_meta_df = add_hashed_features(tuple_baby_meta_df)\n",
    "\n",
    "tuple_baby_meta_df.unpersist()\n",
    "hash_baby_meta_df.cache().show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(hash_baby_meta_df.where(hash_baby_meta_df.description_hash_features.isNull()).count())\n",
    "print(hash_baby_meta_df.where(hash_baby_meta_df.title_hash_features.isNull()).count())\n",
    "print(hash_baby_meta_df.where(hash_baby_meta_df.category_hash_features.isNull()).count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+\n",
      "|      asin|            features|\n",
      "+----------+--------------------+\n",
      "|0188399313|(161,[0,1,5,12,16...|\n",
      "|0188399518|(161,[6,26,30,32,...|\n",
      "|0188399399|(161,[1,5,6,11,17...|\n",
      "+----------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.linalg import Vectors, VectorUDT, SparseVector\n",
    "\n",
    "\n",
    "# combine_hash_features_udf = udf(lambda x1:x1, VectorUDT())\n",
    "def combine_hash_features(col1, col2, col3):\n",
    "#   if isinstance(col1,SparseVector) and isinstance(col2,SparseVector) and isinstance(col3,SparseVector):\n",
    "  combined_array = np.concatenate((col1.toArray(),col2.toArray(), col3.toArray()))\n",
    "  sparse_vec = {i:combined_array[i] for i in np.nonzero(combined_array)[0]}\n",
    "  return Vectors.sparse(len(combined_array), sparse_vec)\n",
    "\n",
    "combine_hash_features_udf = udf(combine_hash_features, VectorUDT())\n",
    "\n",
    "hash_combined_baby_meta_df = hash_baby_meta_df.select(hash_baby_meta_df.asin, \\\n",
    "                                                                    combine_hash_features_udf(hash_baby_meta_df.description_hash_features, \\\n",
    "                                                                                              hash_baby_meta_df.title_hash_features, \\\n",
    "                                                                                              hash_baby_meta_df.category_hash_features).alias('features'))\n",
    "hash_baby_meta_df.unpersist()\n",
    "hash_combined_baby_meta_df.cache().show(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(50, 2942484.0851087314), (100, 2820332.5704339067), (250, 2666849.487968004), (500, 2478428.0219850424), (1000, 2277073.932595025), (2000, 2037986.596878447), (4000, 1721131.695778002)]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "num_clusters_list = [50,100,250, 500, 1000, 2000, 4000]\n",
    "scores = []\n",
    "models = []\n",
    "for num_clusters in num_clusters_list:\n",
    "    kmeans = KMeans(k=num_clusters, seed=1, featuresCol=\"features\")\n",
    "    model = kmeans.fit(hash_combined_baby_meta_df)\n",
    "    models.append(model)\n",
    "    score = model.computeCost(hash_combined_baby_meta_df)\n",
    "    scores.append((num_clusters, score))\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZQAAAD8CAYAAABQFVIjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQTEzs1K4oJiZWSlcUMzMrBT/H3sP/lCzhkdvAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f1422a98668>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "xs = [pair[0] for pair in scores]\n",
    "ys = [pair[1] for pair in scores]\n",
    "plt.scatter(xs,ys)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_cluster_df(kmeans_spark_df):\n",
    "    return kmeans_spark_df.toPandas()[['asin', 'prediction']]\\\n",
    "            .rename(index=str, columns={\"prediction\": \"clusterId\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "baby_cluster_df = create_cluster_df(models[6].transform(hash_combined_baby_meta_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "baby_cluster_df.to_csv('baby_4000_cluster_df.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  },
  "name": "clustering_clean",
  "notebookId": 380649835595690
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
