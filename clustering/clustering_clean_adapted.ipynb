{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "ratings_df_schema = StructType(\n",
    "    [StructField('asin', StringType()),\n",
    "    StructField('helpful', ArrayType(IntegerType())),\n",
    "    StructField('overall', FloatType()),\n",
    "    StructField('reviewText', StringType()),\n",
    "    StructField('reviewTime', DateType()),\n",
    "    StructField('reviewerID', StringType()),\n",
    "    StructField('summary', StringType())])\n",
    "\n",
    "# StructField('unixReviewTime', LongType())\n",
    "# ['asin', 'description', 'title', 'categories']\n",
    "metadata_df_schema = StructType(\n",
    "    [StructField('asin', StringType()),\n",
    "    StructField('description', StringType()),\n",
    "    StructField('title', StringType()),\n",
    "    StructField('categories', ArrayType(StringType()))])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#import metadata files\n",
    "\n",
    "import gzip\n",
    "from pyspark.sql import Row\n",
    "from collections import OrderedDict\n",
    "from datetime import datetime\n",
    "\n",
    "def parse_metadata(path):\n",
    "    g = gzip.open(path, 'rb')\n",
    "    for l in g:\n",
    "        yield eval(l)\n",
    "\n",
    "def create_metadata_df(path):\n",
    "    metadata = []\n",
    "    for entry in parse_metadata(path):\n",
    "        metadata.append(entry)\n",
    "\n",
    "  #find all keys\n",
    "    all_keys = []\n",
    "    for i in range(len(metadata)):\n",
    "        for key in metadata[i].keys():\n",
    "            if key not in all_keys:\n",
    "                all_keys.append(key)\n",
    "    print(all_keys)\n",
    "  #['asin', 'categories', 'price', 'related'] in instant videos\n",
    "  #['asin', 'imUrl', 'description', 'categories', 'title', 'related', 'price', 'salesRank', 'brand'] in electronics\n",
    "  \n",
    "  #check number of entries that contains each key\n",
    "    key_count_dict = {}\n",
    "    for key in all_keys:\n",
    "        key_count_dict[key] = 0\n",
    "    for i in range(len(metadata)):\n",
    "        for key in all_keys:\n",
    "            if key in metadata[i].keys():\n",
    "                key_count_dict[key] += 1\n",
    "    print(key_count_dict)  \n",
    "#  {'asin': 498196, 'description': 459470, 'title': 491194, 'price': 389693, 'imUrl': 498021, 'related': 366959, 'salesRank': 128706, 'brand': 142532, 'categories': 498196} for electronics\n",
    "    discard_missing_info_col = True\n",
    "    cols_to_keep = ['asin', 'description', 'title', 'categories']\n",
    "    filtered_metadata = []\n",
    "    for i in range(len(metadata)):\n",
    "        keep_col = True\n",
    "        if discard_missing_info_col:\n",
    "      #only retain items that contain info for all columns\n",
    "            cols = [x for x in cols_to_keep if x in metadata[i].keys()]\n",
    "            if len(cols) != len(cols_to_keep):\n",
    "                keep_col = False\n",
    "        if keep_col:\n",
    "            temp_dict = {}\n",
    "            for col in cols_to_keep: \n",
    "                if col not in metadata[i].keys():\n",
    "                    temp_dict[col] = None\n",
    "                elif col == 'categories':#given as array of array so only keeep the inner array\n",
    "                    temp_dict[col] = metadata[i]['categories'][0]\n",
    "                else:\n",
    "                    temp_dict[col] = metadata[i][col]\n",
    "            filtered_metadata.append(temp_dict)\n",
    "\n",
    "    return sqlContext.createDataFrame(filtered_metadata, schema = metadata_df_schema)#schema=metadata_df_schema\n",
    "  \n",
    "\n",
    "# books_df = create_review_df('/dbfs/tmp/Metadata/Books.json.gz')\n",
    "#instant_video_meta_df = create_metadata_df('/dbfs/tmp/Metadata/Instant_Video.json.gz')\n",
    "# movies_df = create_review_df('/dbfs/tmp/Metadata/Movies_and_TV.json.gz')\n",
    "# cds_df = create_review_df('/dbfs/tmp/Metadata/CDs_and_Vinyl.json.gz')\n",
    "# instruments_df = create_review_df('/dbfs/tmp/Metadata/Musical_Instruments.json.gz')\n",
    "# instant_video_df = create_review_df('/dbfs/tmp/Metadata/Instant_Videos.json.gz'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['asin', 'categories', 'description', 'title', 'price', 'imUrl', 'brand', 'related', 'salesRank']\n",
      "{'asin': 71317, 'categories': 71317, 'description': 65642, 'title': 71241, 'price': 57741, 'imUrl': 71243, 'brand': 27858, 'related': 58721, 'salesRank': 36}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[asin: string, description: string, title: string, categories: array<string>]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baby_meta_df = create_metadata_df('spark_notebooks/meta_Baby.json.gz')\n",
    "baby_meta_df.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+--------------------+----------+\n",
      "|      asin|         description|               title|categories|\n",
      "+----------+--------------------+--------------------+----------+\n",
      "|0188399313|Wee-Go Glass baby...|Lifefactory 4oz B...|    [Baby]|\n",
      "|0188399518|The Planet Wise F...|Planetwise Flanne...|    [Baby]|\n",
      "|0188399399|The Planet Wise W...|Planetwise Wipe P...|    [Baby]|\n",
      "|0316967297|Hand crafted set ...|Annas Dream Full ...|    [Baby]|\n",
      "|0615447279|Thumbuddy To Love...|Stop Pacifier Suc...|    [Baby]|\n",
      "|0670062049|A baby can be put...|5 Pink Gumdrops +...|    [Baby]|\n",
      "|0705391752|Overview\n",
      "\n",
      "Include...|A Tale of Baby's ...|    [Baby]|\n",
      "|097293751X|Easily keep track...|Baby Tracker&reg;...|    [Baby]|\n",
      "|0974671517|What should we ca...|Wee Gallery Twins...|    [Baby]|\n",
      "|0980027519|These adorable, h...|Nature's Lullabie...|    [Baby]|\n",
      "|0980027500|This calendar pro...|Nature's Lullabie...|    [Baby]|\n",
      "|0980027586|This extra sticke...|Nature's Lullabie...|    [Baby]|\n",
      "|0980027594|This calendar pro...|Nature's Lullabie...|    [Baby]|\n",
      "|0981257224|The 7.53x5.53 spi...|Baby's First Jour...|    [Baby]|\n",
      "|0983676534|Introducing the a...|The Letter Heads ...|    [Baby]|\n",
      "|0985072830|This truly one of...|Islamic Wall Cloc...|    [Baby]|\n",
      "|1059879875|Become an interio...|Modern House Find...|    [Baby]|\n",
      "|1059875748|Become an interio...|Modern House Kids...|    [Baby]|\n",
      "|1432109529|Chronicle the eve...|&quot;My First Ye...|    [Baby]|\n",
      "|1450803229|More than 250 Acc...|Modern Baby Scrap...|    [Baby]|\n",
      "+----------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "baby_meta_df.show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import pandas as pd\n",
    "#df1 = pd.read_csv('spark_notebooks/baby_product_summary.csv')[['asin','rating_count']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#rating_count_df = sqlContext.createDataFrame(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rating_count_df = (sqlContext.read.format(\"com.databricks.spark.csv\").option(\"header\", \"true\")\\\n",
    "                  .load(\"spark_notebooks/baby_rating_count.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "baby_meta_rc_df = baby_meta_df.join(rating_count_df, baby_meta_df['asin']==rating_count_df['asin'], 'inner')\\\n",
    "                    .drop(rating_count_df.asin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filtered_baby_meta_df = baby_meta_rc_df.filter(baby_meta_rc_df.rating_count>=5).drop('rating_count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if baby_meta_df.is_cached:\n",
    "    baby_meta_df.unpersist()\n",
    "if not filtered_baby_meta_df.is_cached:\n",
    "    filtered_baby_meta_df.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2969\n"
     ]
    }
   ],
   "source": [
    "def find_brands(path):\n",
    "    brands = []\n",
    "    count = 0\n",
    "    for entry in parse_metadata(path):\n",
    "        if 'brand' in entry.keys() and entry['brand'] not in brands:\n",
    "            brands.append(entry['brand'])\n",
    "    #brands.pop(brands.index(\"\"))\n",
    "    return brands\n",
    "   \n",
    "brands_stop_words = find_brands('spark_notebooks/meta_Baby.json.gz')\n",
    "print(len(brands_stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "#print # of distinct categories among all products\n",
    "print(baby_meta_df.select('categories').distinct().count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "123\n",
      "['a', 'able', 'about', 'across', 'after', 'all', 'almost', 'also', 'am', 'among', 'an', 'and', 'any', 'are', 'as', 'at', 'be', 'because', 'been', 'but', 'by', 'can', 'cannot', 'could', 'dear', 'did', 'do', 'does', 'either', 'else', 'ever', 'every', 'for', 'from', 'get', 'got', 'had', 'has', 'have', 'he', 'her', 'hers', 'him', 'his', 'how', 'however', 'i', 'if', 'in', 'into', 'is', 'it', 'its', 'just', 'least', 'let', 'like', 'likely', 'may', 'me', 'might', 'most', 'must', 'my', 'neither', 'no', 'nor', 'not', 'of', 'off', 'often', 'on', 'only', 'or', 'other', 'our', 'own', 'rather', 'said', 'say', 'says', 'she', 'should', 'since', 'so', 'some', 'than', 'that', 'the', 'their', 'them', 'then', 'there', 'these', 'they', 'this', 'tis', 'to', 'too', 'twas', 'us', 'wants', 'was', 'we', 'were', 'what', 'when', 'where', 'which', 'while', 'who', 'whom', 'why', 'will', 'with', 'would', 'yet', 'you', 'your', '\\t', '\\n', 's', 't']\n"
     ]
    }
   ],
   "source": [
    "#import stopwords\n",
    "stopwords_path = 'spark_notebooks/stopwords_eng.txt'\n",
    "stopwords_rdd = sc.textFile(stopwords_path)\n",
    "stopwords = stopwords_rdd.collect()\n",
    "stopwords.append('\\t')\n",
    "stopwords.append('\\n')\n",
    "stopwords.append('s')\n",
    "stopwords.append('t')\n",
    "print(len(stopwords))\n",
    "print(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#parse each column by itself instead\n",
    "# feature hashing\n",
    "#convert each row of metadata_df to list of tuple format \n",
    "from pyspark.sql.functions import udf, array, split\n",
    "import re\n",
    "\n",
    "def parse_row(col):\n",
    "    print('a')\n",
    "\n",
    "    if col == None:\n",
    "        return [(0, 'EmptyString')]\n",
    "    elif isinstance(col, str):\n",
    "        #remove brands first\n",
    "        for brand in brands_stop_words:\n",
    "            if brand in col:\n",
    "                col = col.replace(brand, \"\")\n",
    "                break\n",
    "        \n",
    "        words = re.split(r'\\W+',col)\n",
    "        \n",
    "    elif type(col) is list:\n",
    "        words = col\n",
    "  #assign featureID to its index in all_words (e.g. des_words_ID = 0, title_words_ID = 1, etc.)\n",
    "    tuple_list = []\n",
    "    for word in words:\n",
    "        lower_case_word = word.lower()\n",
    "        if lower_case_word not in stopwords:\n",
    "            tuple_list.append((0, lower_case_word))\n",
    "    return tuple_list\n",
    "\n",
    "# parse_row(filtered_baby_meta_df.select('description').first()[0])\n",
    "# filtered_baby_meta_df.select('description').show(1)\n",
    "    \n",
    "parse_row_udf = udf(parse_row, ArrayType(StructType([StructField('_1', LongType()),\n",
    "                                                         StructField('_2', StringType())])))\n",
    "tuple_baby_meta_df = filtered_baby_meta_df.select(filtered_baby_meta_df.asin, \\\n",
    "                                                       parse_row_udf(filtered_baby_meta_df.description).alias('description_features'), \\\n",
    "                                                       parse_row_udf(filtered_baby_meta_df.title).alias('title_features'), \\\n",
    "                                                       parse_row_udf(filtered_baby_meta_df.categories).alias('category_features'))\n",
    "                                                                                                                    \n",
    "if filtered_baby_meta_df.is_cached:\n",
    "    filtered_baby_meta_df.unpersist()\n",
    "if not tuple_baby_meta_df.is_cached:\n",
    "    tuple_baby_meta_df.cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+--------------------+-----------------+\n",
      "|      asin|description_features|      title_features|category_features|\n",
      "+----------+--------------------+--------------------+-----------------+\n",
      "|0615447279|[[0,thumbuddy], [...|[[0,stop], [0,pac...|       [[0,baby]]|\n",
      "|097293751X|[[0,easily], [0,k...|[[0,baby], [0,tra...|       [[0,baby]]|\n",
      "|0980027500|[[0,calendar], [0...|[[0,nature], [0,l...|       [[0,baby]]|\n",
      "+----------+--------------------+--------------------+-----------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tuple_baby_meta_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "#check for null elements for each feature\n",
    "print(tuple_baby_meta_df.where(tuple_baby_meta_df.description_features.isNull()).count())\n",
    "print(tuple_baby_meta_df.where(tuple_baby_meta_df.title_features.isNull()).count())\n",
    "print(tuple_baby_meta_df.where(tuple_baby_meta_df.category_features.isNull()).count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reduce(func, seq, initial):\n",
    "    fix = initial\n",
    "    for item in seq:\n",
    "        fix = func(fix,item)\n",
    "    return fix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import hashlib\n",
    "\n",
    "def hash_function(raw_feats, num_buckets, print_mapping=False):\n",
    "    \"\"\"Calculate a feature dictionary for an observation's features based on hashing.\n",
    "\n",
    "    Note:\n",
    "        Use print_mapping=True for debug purposes and to better understand how the hashing works.\n",
    "\n",
    "    Args:\n",
    "        raw_feats (list of (int, str)): A list of features for an observation.  Represented as\n",
    "            (featureID, value) tuples.\n",
    "        num_buckets (int): Number of buckets to use as features.\n",
    "        print_mapping (bool, optional): If true, the mappings of featureString to index will be\n",
    "            printed.\n",
    "\n",
    "    Returns:\n",
    "        dict of int to float:  The keys will be integers which represent the buckets that the\n",
    "            features have been hashed to.  The value for a given key will contain the count of the\n",
    "            (featureID, value) tuples that have hashed to that key.\n",
    "    \"\"\"\n",
    "    mapping = { category + ':' + str(ind):\n",
    "                int(int(hashlib.md5((category + ':' + str(ind)).encode('utf-8')).hexdigest(), 16) % num_buckets)\n",
    "                for ind, category in raw_feats}\n",
    "    if(print_mapping): print(mapping)\n",
    "\n",
    "    def map_update(l, r):\n",
    "        l[r] += 1.0\n",
    "        return l\n",
    "\n",
    "    sparse_features = reduce(map_update, mapping.values(), defaultdict(float))\n",
    "    return dict(sparse_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------------+--------------------+----------------------+\n",
      "|      asin|description_hash_features| title_hash_features|category_hash_features|\n",
      "+----------+-------------------------+--------------------+----------------------+\n",
      "|0615447279|     (128,[7,12,18,20,...|(128,[12,23,28,57...|         (1,[0],[1.0])|\n",
      "|097293751X|     (128,[0,1,2,3,4,5...|(128,[4,26,45,48,...|         (1,[0],[1.0])|\n",
      "|0980027500|     (128,[1,7,24,25,2...|(128,[12,37,99,10...|         (1,[0],[1.0])|\n",
      "+----------+-------------------------+--------------------+----------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "num_hash_buckets_description = 2 ** 7\n",
    "num_hash_buckets_title = 2 ** 7\n",
    "num_hash_buckets_category = 2 ** 0\n",
    "\n",
    "# UDF that returns a vector of hashed features given an Array of tuples\n",
    "\n",
    "def tuples_to_hash_features_desc(col):\n",
    "  return Vectors.sparse(num_hash_buckets_description, hash_function(col, num_hash_buckets_description))\n",
    "tuples_to_hash_features_desc_udf = udf(tuples_to_hash_features_desc, VectorUDT())\n",
    "\n",
    "def tuples_to_hash_features_title(col):\n",
    "  return Vectors.sparse(num_hash_buckets_title, hash_function(col, num_hash_buckets_title))\n",
    "tuples_to_hash_features_title_udf = udf(tuples_to_hash_features_title, VectorUDT())\n",
    "\n",
    "def tuples_to_hash_features_category(col):\n",
    "  return Vectors.sparse(num_hash_buckets_category, hash_function(col, num_hash_buckets_category))\n",
    "tuples_to_hash_features_category_udf = udf(tuples_to_hash_features_category, VectorUDT())\n",
    "\n",
    "\n",
    "def add_hashed_features(df):\n",
    "    \"\"\"Return a DataFrame with labels and hashed features.\n",
    "\n",
    "    Note:\n",
    "        Make sure to cache the DataFrame that you are returning.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame with 'tuples' column): A DataFrame containing the tuples to be hashed.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: A DataFrame with a 'label' column and a 'features' column that contains a\n",
    "            SparseVector of hashed features.\n",
    "    \"\"\"\n",
    "    return (df.select(df.asin, \\\n",
    "                      tuples_to_hash_features_desc_udf(df.description_features).alias('description_hash_features'), \\\n",
    "                      tuples_to_hash_features_title_udf(df.title_features).alias('title_hash_features'), \\\n",
    "                      tuples_to_hash_features_category_udf(df.category_features).alias('category_hash_features')))\n",
    "\n",
    "hash_baby_meta_df = add_hashed_features(tuple_baby_meta_df)\n",
    "\n",
    "tuple_baby_meta_df.unpersist()\n",
    "hash_baby_meta_df.cache().show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(hash_baby_meta_df.where(hash_baby_meta_df.description_hash_features.isNull()).count())\n",
    "print(hash_baby_meta_df.where(hash_baby_meta_df.title_hash_features.isNull()).count())\n",
    "print(hash_baby_meta_df.where(hash_baby_meta_df.category_hash_features.isNull()).count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+\n",
      "|      asin|            features|\n",
      "+----------+--------------------+\n",
      "|0615447279|(257,[7,12,18,20,...|\n",
      "|097293751X|(257,[0,1,2,3,4,5...|\n",
      "|0980027500|(257,[1,7,24,25,2...|\n",
      "+----------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.linalg import Vectors, VectorUDT, SparseVector\n",
    "\n",
    "\n",
    "# combine_hash_features_udf = udf(lambda x1:x1, VectorUDT())\n",
    "def combine_hash_features(col1, col2, col3):\n",
    "#   if isinstance(col1,SparseVector) and isinstance(col2,SparseVector) and isinstance(col3,SparseVector):\n",
    "  combined_array = np.concatenate((col1.toArray(),col2.toArray(), col3.toArray()))\n",
    "  sparse_vec = {i:combined_array[i] for i in np.nonzero(combined_array)[0]}\n",
    "  return Vectors.sparse(len(combined_array), sparse_vec)\n",
    "\n",
    "combine_hash_features_udf = udf(combine_hash_features, VectorUDT())\n",
    "\n",
    "hash_combined_baby_meta_df = hash_baby_meta_df.select(hash_baby_meta_df.asin, \\\n",
    "                                                                    combine_hash_features_udf(hash_baby_meta_df.description_hash_features, \\\n",
    "                                                                                              hash_baby_meta_df.title_hash_features, \\\n",
    "                                                                                              hash_baby_meta_df.category_hash_features).alias('features'))\n",
    "hash_baby_meta_df.unpersist()\n",
    "hash_combined_baby_meta_df.cache().show(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1000, 654634.6037635534), (1500, 592536.7144837584), (2000, 542256.0533069895)]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "num_clusters_list = [1000, 1500, 2000]\n",
    "scores = []\n",
    "models = []\n",
    "for num_clusters in num_clusters_list:\n",
    "    kmeans = KMeans(k=num_clusters, seed=1, featuresCol=\"features\")\n",
    "    model = kmeans.fit(hash_combined_baby_meta_df)\n",
    "    models.append(model)\n",
    "    score = model.computeCost(hash_combined_baby_meta_df)\n",
    "    scores.append((num_clusters, score))\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY0AAAD8CAYAAACLrvgBAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f294c9afb00>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "xs = [pair[0] for pair in scores]\n",
    "ys = [pair[1] for pair in scores]\n",
    "plt.scatter(xs,ys)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_cluster_df(kmeans_spark_df):\n",
    "    return kmeans_spark_df.toPandas()[['asin', 'prediction']]\\\n",
    "            .rename(index=str, columns={\"prediction\": \"clusterId\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "baby_cluster_df = create_cluster_df(models[2].transform(hash_combined_baby_meta_df))\n",
    "baby_cluster_df.to_csv('spark_notebooks/baby_2000_2_cluster_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "baby_cluster_df = create_cluster_df(models[1].transform(hash_combined_baby_meta_df))\n",
    "baby_cluster_df.to_csv('spark_notebooks/baby_1500_2_cluster_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "baby_cluster_df = create_cluster_df(models[0].transform(hash_combined_baby_meta_df))\n",
    "baby_cluster_df.to_csv('spark_notebooks/baby_1000_2_cluster_df.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  },
  "name": "clustering_clean",
  "notebookId": 380649835595690
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
