{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "ratings_df_schema = StructType(\n",
    "    [StructField('asin', StringType()),\n",
    "    StructField('helpful', ArrayType(IntegerType())),\n",
    "    StructField('overall', FloatType()),\n",
    "    StructField('reviewText', StringType()),\n",
    "    StructField('reviewTime', DateType()),\n",
    "    StructField('reviewerID', StringType()),\n",
    "    StructField('summary', StringType())])\n",
    "\n",
    "# StructField('unixReviewTime', LongType())\n",
    "# ['asin', 'description', 'title', 'categories']\n",
    "metadata_df_schema = StructType(\n",
    "    [StructField('asin', StringType()),\n",
    "    StructField('description', StringType()),\n",
    "    StructField('title', StringType()),\n",
    "    StructField('categories', ArrayType(StringType()))])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#import metadata files\n",
    "\n",
    "import gzip\n",
    "from pyspark.sql import Row\n",
    "from collections import OrderedDict\n",
    "from datetime import datetime\n",
    "\n",
    "def parse_metadata(path):\n",
    "    g = gzip.open(path, 'rb')\n",
    "    for l in g:\n",
    "        yield eval(l)\n",
    "\n",
    "def create_metadata_df(path):\n",
    "    metadata = []\n",
    "    for entry in parse_metadata(path):\n",
    "        metadata.append(entry)\n",
    "\n",
    "  #find all keys\n",
    "    all_keys = []\n",
    "    for i in range(len(metadata)):\n",
    "        for key in metadata[i].keys():\n",
    "            if key not in all_keys:\n",
    "                all_keys.append(key)\n",
    "    print(all_keys)\n",
    "  #['asin', 'categories', 'price', 'related'] in instant videos\n",
    "  #['asin', 'imUrl', 'description', 'categories', 'title', 'related', 'price', 'salesRank', 'brand'] in electronics\n",
    "  \n",
    "  #check number of entries that contains each key\n",
    "    key_count_dict = {}\n",
    "    for key in all_keys:\n",
    "        key_count_dict[key] = 0\n",
    "    for i in range(len(metadata)):\n",
    "        for key in all_keys:\n",
    "            if key in metadata[i].keys():\n",
    "                key_count_dict[key] += 1\n",
    "    print(key_count_dict)  \n",
    "#  {'asin': 498196, 'description': 459470, 'title': 491194, 'price': 389693, 'imUrl': 498021, 'related': 366959, 'salesRank': 128706, 'brand': 142532, 'categories': 498196} for electronics\n",
    "    discard_missing_info_col = True\n",
    "    cols_to_keep = ['asin', 'description', 'title', 'categories']\n",
    "    filtered_metadata = []\n",
    "    for i in range(len(metadata)):\n",
    "        keep_col = True\n",
    "        if discard_missing_info_col:\n",
    "      #only retain items that contain info for all columns\n",
    "            cols = [x for x in cols_to_keep if x in metadata[i].keys()]\n",
    "            if len(cols) != len(cols_to_keep):\n",
    "                keep_col = False\n",
    "        if keep_col:\n",
    "            temp_dict = {}\n",
    "            for col in cols_to_keep: \n",
    "                if col not in metadata[i].keys():\n",
    "                    temp_dict[col] = None\n",
    "                elif col == 'categories':#given as array of array so only keeep the inner array\n",
    "                    temp_dict[col] = metadata[i]['categories'][0]\n",
    "                else:\n",
    "                    temp_dict[col] = metadata[i][col]\n",
    "            filtered_metadata.append(temp_dict)\n",
    "\n",
    "    return sqlContext.createDataFrame(filtered_metadata, schema = metadata_df_schema)#schema=metadata_df_schema\n",
    "  \n",
    "\n",
    "# books_df = create_review_df('/dbfs/tmp/Metadata/Books.json.gz')\n",
    "#instant_video_meta_df = create_metadata_df('/dbfs/tmp/Metadata/Instant_Video.json.gz')\n",
    "# movies_df = create_review_df('/dbfs/tmp/Metadata/Movies_and_TV.json.gz')\n",
    "# cds_df = create_review_df('/dbfs/tmp/Metadata/CDs_and_Vinyl.json.gz')\n",
    "# instruments_df = create_review_df('/dbfs/tmp/Metadata/Musical_Instruments.json.gz')\n",
    "# instant_video_df = create_review_df('/dbfs/tmp/Metadata/Instant_Videos.json.gz'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['asin', 'categories', 'description', 'title', 'price', 'imUrl', 'brand', 'related', 'salesRank']\n",
      "{'asin': 71317, 'categories': 71317, 'description': 65642, 'title': 71241, 'price': 57741, 'imUrl': 71243, 'brand': 27858, 'related': 58721, 'salesRank': 36}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[asin: string, description: string, title: string, categories: array<string>]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baby_meta_df = create_metadata_df('spark_notebooks/meta_Baby.json.gz')\n",
    "baby_meta_df.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+--------------------+----------+\n",
      "|      asin|         description|               title|categories|\n",
      "+----------+--------------------+--------------------+----------+\n",
      "|0188399313|Wee-Go Glass baby...|Lifefactory 4oz B...|    [Baby]|\n",
      "|0188399518|The Planet Wise F...|Planetwise Flanne...|    [Baby]|\n",
      "|0188399399|The Planet Wise W...|Planetwise Wipe P...|    [Baby]|\n",
      "+----------+--------------------+--------------------+----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "baby_meta_df.show(3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import pandas as pd\n",
    "#df1 = pd.read_csv('spark_notebooks/baby_product_summary.csv')[['asin','rating_count']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#rating_count_df = sqlContext.createDataFrame(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rating_count_df = (sqlContext.read.format(\"com.databricks.spark.csv\").option(\"header\", \"true\")\\\n",
    "                  .load(\"spark_notebooks/baby_rating_count.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "baby_meta_rc_df = baby_meta_df.join(rating_count_df, baby_meta_df['asin']==rating_count_df['asin'], 'inner')\\\n",
    "                    .drop(rating_count_df.asin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+--------------------+----------+---+------------+\n",
      "|      asin|         description|               title|categories|_c0|rating_count|\n",
      "+----------+--------------------+--------------------+----------+---+------------+\n",
      "|0188399313|Wee-Go Glass baby...|Lifefactory 4oz B...|    [Baby]|  0|         1.0|\n",
      "|0188399518|The Planet Wise F...|Planetwise Flanne...|    [Baby]|  1|         2.0|\n",
      "|0188399399|The Planet Wise W...|Planetwise Wipe P...|    [Baby]|  2|         1.0|\n",
      "|0316967297|Hand crafted set ...|Annas Dream Full ...|    [Baby]|  3|         2.0|\n",
      "|0615447279|Thumbuddy To Love...|Stop Pacifier Suc...|    [Baby]|  4|         9.0|\n",
      "|0670062049|A baby can be put...|5 Pink Gumdrops +...|    [Baby]|  5|         1.0|\n",
      "|0705391752|Overview\n",
      "\n",
      "Include...|A Tale of Baby's ...|    [Baby]|  6|         3.0|\n",
      "|097293751X|Easily keep track...|Baby Tracker&reg;...|    [Baby]|  7|        42.0|\n",
      "|0974671517|What should we ca...|Wee Gallery Twins...|    [Baby]|  8|         2.0|\n",
      "|0980027519|These adorable, h...|Nature's Lullabie...|    [Baby]|  9|         2.0|\n",
      "|0980027500|This calendar pro...|Nature's Lullabie...|    [Baby]| 10|        12.0|\n",
      "|0980027586|This extra sticke...|Nature's Lullabie...|    [Baby]| 11|         8.0|\n",
      "|0980027594|This calendar pro...|Nature's Lullabie...|    [Baby]| 12|        31.0|\n",
      "|0981257224|The 7.53x5.53 spi...|Baby's First Jour...|    [Baby]| 13|         4.0|\n",
      "|0983676534|Introducing the a...|The Letter Heads ...|    [Baby]| 14|         1.0|\n",
      "|0985072830|This truly one of...|Islamic Wall Cloc...|    [Baby]| 15|         1.0|\n",
      "|1059879875|Become an interio...|Modern House Find...|    [Baby]| 16|         1.0|\n",
      "|1059875748|Become an interio...|Modern House Kids...|    [Baby]| 17|         2.0|\n",
      "|1432109529|Chronicle the eve...|&quot;My First Ye...|    [Baby]| 19|         1.0|\n",
      "|1450803229|More than 250 Acc...|Modern Baby Scrap...|    [Baby]| 20|         3.0|\n",
      "+----------+--------------------+--------------------+----------+---+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "baby_meta_rc_df .show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filtered_baby_meta_df = baby_meta_rc_df.filter(baby_meta_rc_df.rating_count>=5).drop('rating_count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if baby_meta_df.is_cached:\n",
    "    baby_meta_df.unpersist()\n",
    "if not filtered_baby_meta_df.is_cached:\n",
    "    filtered_baby_meta_df.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2968\n"
     ]
    }
   ],
   "source": [
    "def find_brands(path):\n",
    "    brands = []\n",
    "    count = 0\n",
    "    for entry in parse_metadata(path):\n",
    "        if 'brand' in entry.keys() and entry['brand'] not in brands:\n",
    "            brands.append(entry['brand'])\n",
    "    brands.pop(brands.index(\"\"))\n",
    "    return brands\n",
    "   \n",
    "brands_stop_words = find_brands('spark_notebooks/meta_Baby.json.gz')\n",
    "print(len(brands_stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "#print # of distinct categories among all products\n",
    "print(baby_meta_df.select('categories').distinct().count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "119\n",
      "['a', 'able', 'about', 'across', 'after', 'all', 'almost', 'also', 'am', 'among', 'an', 'and', 'any', 'are', 'as', 'at', 'be', 'because', 'been', 'but', 'by', 'can', 'cannot', 'could', 'dear', 'did', 'do', 'does', 'either', 'else', 'ever', 'every', 'for', 'from', 'get', 'got', 'had', 'has', 'have', 'he', 'her', 'hers', 'him', 'his', 'how', 'however', 'i', 'if', 'in', 'into', 'is', 'it', 'its', 'just', 'least', 'let', 'like', 'likely', 'may', 'me', 'might', 'most', 'must', 'my', 'neither', 'no', 'nor', 'not', 'of', 'off', 'often', 'on', 'only', 'or', 'other', 'our', 'own', 'rather', 'said', 'say', 'says', 'she', 'should', 'since', 'so', 'some', 'than', 'that', 'the', 'their', 'them', 'then', 'there', 'these', 'they', 'this', 'tis', 'to', 'too', 'twas', 'us', 'wants', 'was', 'we', 'were', 'what', 'when', 'where', 'which', 'while', 'who', 'whom', 'why', 'will', 'with', 'would', 'yet', 'you', 'your']\n"
     ]
    }
   ],
   "source": [
    "#import stopwords\n",
    "stopwords_path = 'spark_notebooks/stopwords_eng.txt'\n",
    "stopwords_rdd = sc.textFile(stopwords_path)\n",
    "stopwords = stopwords_rdd.collect()\n",
    "print(len(stopwords))\n",
    "print(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#parse each column by itself instead\n",
    "# feature hashing\n",
    "#convert each row of metadata_df to list of tuple format \n",
    "from pyspark.sql.functions import udf, array, split\n",
    "import re\n",
    "\n",
    "def parse_row(col):\n",
    "    if col == None:\n",
    "        return [(0, 'EmptyString')]\n",
    "    elif isinstance(col, str):\n",
    "        #remove brands first\n",
    "        for brand in brands_stop_words:\n",
    "            if brand in col:\n",
    "                col = col.replace(brand, \"\")\n",
    "                break\n",
    "        \n",
    "        words = re.split(r'\\W+',col)\n",
    "        \n",
    "    elif type(col) is list:\n",
    "        words = col\n",
    "   \n",
    "  #assign featureID to its index in all_words (e.g. des_words_ID = 0, title_words_ID = 1, etc.)\n",
    "    tuple_list = []\n",
    "    for word in words:\n",
    "        lower_case_word = word.lower()\n",
    "        if lower_case_word not in stopwords:\n",
    "            tuple_list.append((0, lower_case_word))\n",
    "    return tuple_list\n",
    "\n",
    "  \n",
    "parse_row_udf = udf(parse_row, ArrayType(StructType([StructField('_1', LongType()),\n",
    "                                                         StructField('_2', StringType())])))\n",
    "tuple_baby_meta_df = filtered_baby_meta_df.select(filtered_baby_meta_df.asin, \\\n",
    "                                                       parse_row_udf(filtered_baby_meta_df.description).alias('description_features'), \\\n",
    "                                                       parse_row_udf(filtered_baby_meta_df.title).alias('title_features'), \\\n",
    "                                                       parse_row_udf(filtered_baby_meta_df.categories).alias('category_features'))\n",
    "                                                                                                                    \n",
    "if filtered_baby_meta_df.is_cached:\n",
    "    filtered_baby_meta_df.unpersist()\n",
    "if not tuple_baby_meta_df.is_cached:\n",
    "    tuple_baby_meta_df.cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+--------------------+-----------------+\n",
      "|      asin|description_features|      title_features|category_features|\n",
      "+----------+--------------------+--------------------+-----------------+\n",
      "|0615447279|[[0,thumbuddy], [...|[[0,stop], [0,pac...|       [[0,baby]]|\n",
      "|097293751X|[[0,easily], [0,k...|[[0,], [0,tracker...|       [[0,baby]]|\n",
      "|0980027500|[[0,calendar], [0...|[[0,nature], [0,s...|       [[0,baby]]|\n",
      "+----------+--------------------+--------------------+-----------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tuple_baby_meta_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "#check for null elements for each feature\n",
    "print(tuple_baby_meta_df.where(tuple_baby_meta_df.description_features.isNull()).count())\n",
    "print(tuple_baby_meta_df.where(tuple_baby_meta_df.title_features.isNull()).count())\n",
    "print(tuple_baby_meta_df.where(tuple_baby_meta_df.category_features.isNull()).count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reduce(func, seq, initial):\n",
    "    fix = initial\n",
    "    for item in seq:\n",
    "        fix = func(fix,item)\n",
    "    return fix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import hashlib\n",
    "\n",
    "def hash_function(raw_feats, num_buckets, print_mapping=False):\n",
    "    \"\"\"Calculate a feature dictionary for an observation's features based on hashing.\n",
    "\n",
    "    Note:\n",
    "        Use print_mapping=True for debug purposes and to better understand how the hashing works.\n",
    "\n",
    "    Args:\n",
    "        raw_feats (list of (int, str)): A list of features for an observation.  Represented as\n",
    "            (featureID, value) tuples.\n",
    "        num_buckets (int): Number of buckets to use as features.\n",
    "        print_mapping (bool, optional): If true, the mappings of featureString to index will be\n",
    "            printed.\n",
    "\n",
    "    Returns:\n",
    "        dict of int to float:  The keys will be integers which represent the buckets that the\n",
    "            features have been hashed to.  The value for a given key will contain the count of the\n",
    "            (featureID, value) tuples that have hashed to that key.\n",
    "    \"\"\"\n",
    "    mapping = { category + ':' + str(ind):\n",
    "                int(int(hashlib.md5((category + ':' + str(ind)).encode('utf-8')).hexdigest(), 16) % num_buckets)\n",
    "                for ind, category in raw_feats}\n",
    "    if(print_mapping): print(mapping)\n",
    "\n",
    "    def map_update(l, r):\n",
    "        l[r] += 1.0\n",
    "        return l\n",
    "\n",
    "    sparse_features = reduce(map_update, mapping.values(), defaultdict(float))\n",
    "    return dict(sparse_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------------+--------------------+----------------------+\n",
      "|      asin|description_hash_features| title_hash_features|category_hash_features|\n",
      "+----------+-------------------------+--------------------+----------------------+\n",
      "|0615447279|     (32,[0,1,2,3,4,5,...|(32,[0,3,5,11,12,...|         (1,[0],[1.0])|\n",
      "|097293751X|     (32,[0,1,2,3,4,5,...|(32,[2,4,11,13,16...|         (1,[0],[1.0])|\n",
      "|0980027500|     (32,[1,3,4,5,6,7,...|(32,[3,5,7,12,14,...|         (1,[0],[1.0])|\n",
      "+----------+-------------------------+--------------------+----------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "num_hash_buckets_description = 2 ** 5\n",
    "num_hash_buckets_title = 2 ** 5\n",
    "num_hash_buckets_category = 2 ** 0\n",
    "\n",
    "# UDF that returns a vector of hashed features given an Array of tuples\n",
    "\n",
    "def tuples_to_hash_features_desc(col):\n",
    "  return Vectors.sparse(num_hash_buckets_description, hash_function(col, num_hash_buckets_description))\n",
    "tuples_to_hash_features_desc_udf = udf(tuples_to_hash_features_desc, VectorUDT())\n",
    "\n",
    "def tuples_to_hash_features_title(col):\n",
    "  return Vectors.sparse(num_hash_buckets_title, hash_function(col, num_hash_buckets_title))\n",
    "tuples_to_hash_features_title_udf = udf(tuples_to_hash_features_title, VectorUDT())\n",
    "\n",
    "def tuples_to_hash_features_category(col):\n",
    "  return Vectors.sparse(num_hash_buckets_category, hash_function(col, num_hash_buckets_category))\n",
    "tuples_to_hash_features_category_udf = udf(tuples_to_hash_features_category, VectorUDT())\n",
    "\n",
    "\n",
    "def add_hashed_features(df):\n",
    "    \"\"\"Return a DataFrame with labels and hashed features.\n",
    "\n",
    "    Note:\n",
    "        Make sure to cache the DataFrame that you are returning.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame with 'tuples' column): A DataFrame containing the tuples to be hashed.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: A DataFrame with a 'label' column and a 'features' column that contains a\n",
    "            SparseVector of hashed features.\n",
    "    \"\"\"\n",
    "    return (df.select(df.asin, \\\n",
    "                      tuples_to_hash_features_desc_udf(df.description_features).alias('description_hash_features'), \\\n",
    "                      tuples_to_hash_features_title_udf(df.title_features).alias('title_hash_features'), \\\n",
    "                      tuples_to_hash_features_category_udf(df.category_features).alias('category_hash_features')))\n",
    "\n",
    "hash_baby_meta_df = add_hashed_features(tuple_baby_meta_df)\n",
    "\n",
    "tuple_baby_meta_df.unpersist()\n",
    "hash_baby_meta_df.cache().show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(hash_baby_meta_df.where(hash_baby_meta_df.description_hash_features.isNull()).count())\n",
    "print(hash_baby_meta_df.where(hash_baby_meta_df.title_hash_features.isNull()).count())\n",
    "print(hash_baby_meta_df.where(hash_baby_meta_df.category_hash_features.isNull()).count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+\n",
      "|      asin|            features|\n",
      "+----------+--------------------+\n",
      "|0615447279|(65,[0,1,2,3,4,5,...|\n",
      "|097293751X|(65,[0,1,2,3,4,5,...|\n",
      "|0980027500|(65,[1,3,4,5,6,7,...|\n",
      "+----------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.linalg import Vectors, VectorUDT, SparseVector\n",
    "\n",
    "\n",
    "# combine_hash_features_udf = udf(lambda x1:x1, VectorUDT())\n",
    "def combine_hash_features(col1, col2, col3):\n",
    "#   if isinstance(col1,SparseVector) and isinstance(col2,SparseVector) and isinstance(col3,SparseVector):\n",
    "  combined_array = np.concatenate((col1.toArray(),col2.toArray(), col3.toArray()))\n",
    "  sparse_vec = {i:combined_array[i] for i in np.nonzero(combined_array)[0]}\n",
    "  return Vectors.sparse(len(combined_array), sparse_vec)\n",
    "\n",
    "combine_hash_features_udf = udf(combine_hash_features, VectorUDT())\n",
    "\n",
    "hash_combined_baby_meta_df = hash_baby_meta_df.select(hash_baby_meta_df.asin, \\\n",
    "                                                                    combine_hash_features_udf(hash_baby_meta_df.description_hash_features, \\\n",
    "                                                                                              hash_baby_meta_df.title_hash_features, \\\n",
    "                                                                                              hash_baby_meta_df.category_hash_features).alias('features'))\n",
    "hash_baby_meta_df.unpersist()\n",
    "hash_combined_baby_meta_df.cache().show(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(50, 913582.5452731152), (100, 855736.2414140984), (250, 778209.1963975894), (500, 707582.2173331558), (1000, 624551.9892985307), (2000, 516104.70621416345), (4000, 381047.3486416134)]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "num_clusters_list = [50,100,250, 500, 1000, 2000, 4000]\n",
    "scores = []\n",
    "models = []\n",
    "for num_clusters in num_clusters_list:\n",
    "    kmeans = KMeans(k=num_clusters, seed=1, featuresCol=\"features\")\n",
    "    model = kmeans.fit(hash_combined_baby_meta_df)\n",
    "    models.append(model)\n",
    "    score = model.computeCost(hash_combined_baby_meta_df)\n",
    "    scores.append((num_clusters, score))\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY0AAAD8CAYAAACLrvgBAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f9a2d9af668>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "xs = [pair[0] for pair in scores]\n",
    "ys = [pair[1] for pair in scores]\n",
    "plt.scatter(xs,ys)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_cluster_df(kmeans_spark_df):\n",
    "    return kmeans_spark_df.toPandas()[['asin', 'prediction']]\\\n",
    "            .rename(index=str, columns={\"prediction\": \"clusterId\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "baby_cluster_df = create_cluster_df(models[6].transform(hash_combined_baby_meta_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "baby_cluster_df.to_csv('spark_notebooks/baby_4000_cluster_df.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  },
  "name": "clustering_clean",
  "notebookId": 380649835595690
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
