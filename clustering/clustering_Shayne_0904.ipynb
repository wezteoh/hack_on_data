{"cells":[{"cell_type":"code","source":["# url_list = [('http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Musical_Instruments_5.json.gz', 'Musical_Instruments'),\n#            ('http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Amazon_Instant_Video_5.json.gz', 'Instant_Videos')]\nreview_list = [('http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Books_5.json.gz','Books'),\n       ('http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Electronics_5.json.gz','Electronics'),\n       ('http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Movies_and_TV_5.json.gz','Movies_and_TV'),\n       ('http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_CDs_and_Vinyl_5.json.gz','CDs_and_Vinyl')]\n#metadata_list = [('http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/meta_Amazon_Instant_Video.json.gz', 'Instant_Video')]\nmetadata_list = [('http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/meta_Electronics.json.gz', 'Electronics')]\n\nstop_words_url = 'http://tacit.usc.edu/resources/stopwords_eng.txt'"],"metadata":{},"outputs":[],"execution_count":1},{"cell_type":"code","source":["import urllib\nload_data = False\nload_metadata = False\nload_stop_words = False\n\nif load_data:\n  for url_tuple in review_list:\n    url = url_tuple[0]\n    folder_path = '/tmp/' + url_tuple[1] + '.json.gz'\n    urllib.urlretrieve(url, folder_path)\n    dbutils.fs.mv('file:' + folder_path, 'dbfs:/tmp/Data/' + url_tuple[1] + '.json.gz')\n    display(dbutils.fs.ls(\"dbfs:/tmp/Data\"))\n    \nif load_metadata:\n  for url_tuple in metadata_list:\n    url = url_tuple[0]\n    folder_path = '/tmp/' + url_tuple[1] + '.json.gz'\n    urllib.urlretrieve(url, folder_path)\n    dbutils.fs.mv('file:' + folder_path, 'dbfs:/tmp/Metadata/' + url_tuple[1] + '.json.gz')\n    display(dbutils.fs.ls(\"dbfs:/tmp/Metadata\"))\n    \nif load_stop_words:\n  urllib.urlretrieve(stop_words_url, '/tmp/stopwords_eng.txt')\n  dbutils.fs.mv('file:/tmp/stopwords_eng.txt', 'dbfs:/tmp/stopwords_eng.txt')\n  #   dbutils.fs.mv(\"file:/tmp/test.json.gz\", \"dbfs:/tmp/Data/test.json.gz\")\n  display(dbutils.fs.ls(\"dbfs:/tmp\"))"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["from pyspark.sql.types import *\n\nratings_df_schema = StructType(\n  [StructField('asin', StringType()),\n   StructField('helpful', ArrayType(IntegerType())),\n   StructField('overall', FloatType()),\n   StructField('reviewText', StringType()),\n   StructField('reviewTime', DateType()),\n   StructField('reviewerID', StringType()),\n   StructField('summary', StringType())]\n)\n# StructField('unixReviewTime', LongType())\n# ['asin', 'description', 'title', 'categories']\nmetadata_df_schema = StructType(\n  [StructField('asin', StringType()),\n   StructField('description', StringType()),\n   StructField('title', StringType()),\n   StructField('categories', ArrayType(StringType()))]\n)\n# metadata_df_schema = StructType(\n#   [StructField('Product_ID', StringType()),\n#    StructField('Features', ArrayType(StructType([StructField('_1', LongType()), StructField('_2', ArrayType(StringType()))])))]\n# )\n\n#parse_point_udf = udf(parse_point, ArrayType(StructType([StructField('_1', LongType()),\n#                                                          StructField('_2', StringType())])))\n"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["#import 5-core review data\n\nimport gzip\nfrom pyspark.sql import Row\nfrom collections import OrderedDict\nfrom datetime import datetime\n\ndef parse_review(path):\n  g = gzip.open(path, 'rb')\n  for l in g:\n    yield eval(l)\n\ndef create_review_df(path):\n  ratings = []\n  for review in parse_review(path):\n    ratings.append(review)\n\n  #check if all entries contains a value for each key\n#   missing_keys = []\n#   all_keys = ['reviewerID', 'asin', 'reviewerName', 'helpful', 'unixReviewTime', 'reviewText', 'overall', 'reviewTime', 'summary']\n#   for i in range(len(ratings)):\n#     for j in range(len(all_keys)):\n#       if (all_keys[j] not in ratings[i].keys()):\n#         missing_keys.append(all_keys[j])\n#   print set(missing_keys)\n  #not all entries have 'reviewerName'\n\n  #remove revivewerName, unixreviewTime from all entries and convert reivewTime to datetime.date type\n  for i in range(len(ratings)):\n    ratings[i].pop('reviewerName', None)\n    ratings[i].pop('unixReviewTime', None)\n    ratings[i]['reviewTime'] = datetime.strptime(ratings[i]['reviewTime'], '%m %d, %Y')\n\n  return sqlContext.createDataFrame(ratings, schema=ratings_df_schema)\n  \n# books_df = create_review_df('/dbfs/tmp/Data/Books.json.gz')\nelectronics_df = create_review_df('/dbfs/tmp/Data/Electronics.json.gz')\n# movies_df = create_review_df('/dbfs/tmp/Data/Movies_and_TV.json.gz')\n# cds_df = create_review_df('/dbfs/tmp/Data/CDs_and_Vinyl.json.gz')\n# instruments_df = create_review_df('/dbfs/tmp/Data/Musical_Instruments.json.gz')\n# instant_video_df = create_review_df('/dbfs/tmp/Data/Instant_Videos.json.gz')\n\n\n# print books_df.cache().count()\nprint electronics_df.cache().count()\n# print movies_df.cache().count()\n# print cds_df.cache().count()\n# print instruments_df.cache().count()\n# print instant_video_df.cache().count()\n\n\n\nelectronics_df.show(3, truncate=True)\n\n#alternative way to create dataframe, but each element will be a string\n#def convert_to_row(d):\n  #add 'reviewerName' to dict if missing\n#  if 'reviewerName' not in d.keys():\n#    d['reviewerName'] = 'unknown'\n#  return Row(**OrderedDict(sorted(d.items())))\n#test_df = sc.parallelize(ratings).map(convert_to_row).toDF()\n"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["\n#import metadata files\n\nimport gzip\nfrom pyspark.sql import Row\nfrom collections import OrderedDict\nfrom datetime import datetime\n\ndef parse_metadata(path):\n  g = gzip.open(path, 'rb')\n  for l in g:\n    yield eval(l)\n\ndef create_metadata_df(path):\n  metadata = []\n  for entry in parse_metadata(path):\n    metadata.append(entry)\n\n#   #find all keys\n#   all_keys = []\n#   for i in range(len(metadata)):\n#     for key in metadata[i].keys():\n#       if key not in all_keys:\n#         all_keys.append(key)\n#   print all_keys\n#   #['asin', 'categories', 'price', 'related'] in instant videos\n#   #['asin', 'imUrl', 'description', 'categories', 'title', 'related', 'price', 'salesRank', 'brand'] in electronics\n  \n#   #check number of entries that contains each key\n#   key_count_dict = {}\n#   for key in all_keys:\n#     key_count_dict[key] = 0\n#   for i in range(len(metadata)):\n#     for key in all_keys:\n#       if key in metadata[i].keys():\n#         key_count_dict[key] += 1\n#   print key_count_dict  \n#   #{'asin': 498196, 'description': 459470, 'title': 491194, 'price': 389693, 'imUrl': 498021, 'related': 366959, 'salesRank': 128706, 'brand': 142532, 'categories': 498196} for electronics\n  discard_missing_info_col = False\n  cols_to_keep = ['asin', 'description', 'title', 'categories']\n  filtered_metadata = []\n  for i in range(len(metadata)):\n    keep_col = True\n    if discard_missing_info_col:\n      #only retain items that contain info for all columns\n      cols = [x for x in cols_to_keep if x in metadata[i].keys()]\n      if len(cols) != len(cols_to_keep):\n        keep_col = False\n    if keep_col:\n      temp_dict = {}\n      for col in cols_to_keep: \n        if col not in metadata[i].keys():\n          temp_dict[col] = None\n        elif col == 'categories':#given as array of array so only keeep the inner array\n          temp_dict[col] = metadata[i]['categories'][0]\n        else:\n          temp_dict[col] = metadata[i][col]\n      filtered_metadata.append(temp_dict)\n#       features = [(x, temp_dict[cols_to_keep[x]]) for x in range(len(cols_to_keep))]\n#       print features\n#       filtered_metadata.append((metadata[i][label], features))\n\n  return sqlContext.createDataFrame(filtered_metadata, schema = metadata_df_schema)#schema=metadata_df_schema\n  \n\n# books_df = create_review_df('/dbfs/tmp/Metadata/Books.json.gz')\nelectronics_meta_df = create_metadata_df('/dbfs/tmp/Metadata/Electronics.json.gz')\nelectronics_meta_df.cache().count()\n#instant_video_meta_df = create_metadata_df('/dbfs/tmp/Metadata/Instant_Video.json.gz')\n# movies_df = create_review_df('/dbfs/tmp/Metadata/Movies_and_TV.json.gz')\n# cds_df = create_review_df('/dbfs/tmp/Metadata/CDs_and_Vinyl.json.gz')\n# instruments_df = create_review_df('/dbfs/tmp/Metadata/Musical_Instruments.json.gz')\n# instant_video_df = create_review_df('/dbfs/tmp/Metadata/Instant_Videos.json.gz')\n\n# print books_df.cache().count()\n#print electronics_df.cache().count()\n# print movies_df.cache().count()\n# print cds_df.cache().count()\n# print instruments_df.cache().count()\n# print instant_video_df.cache().count()\n\n\n\n"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["print electronics_meta_df.count()\n\nelectronics_meta_df.show(20)\n\n"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["a = electronics_meta_df.select('title').collect()\nprint type(a[0]['title'])\nprint a[0]['title'].split()\nprint a[0]['title'].split()[0]"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["#import stopwords\nstopwords_path = 'dbfs:/tmp/stopwords_eng.txt'\nstopwords_rdd = sc.textFile(stopwords_path)\nstopwords = stopwords_rdd.collect()\nprint len(stopwords)\nprint stopwords\n"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["# feature hashing\n#convert each row of metadata_df to list of tuple format \nfrom pyspark.sql.functions import udf, array, split\n\ndef parse_row(description, title, categories):\n  #description: string\n  #title: string\n  #categories: list of words\n  des_words = []\n  title_words = []\n  if description != None:\n    des_words = description.split(' ')\n  if title != None:\n    title_words = title.split(' ')\n#   return [(0,description), (1,title), (2, categories[0])]\n\n  all_words = [des_words, title_words, categories]\n\n  #assign featureID to its index in all_words (e.g. des_words_ID = 0, title_words_ID = 1, etc.)\n  tuple_list = []\n  for ind in range(len(all_words)):\n    for word in all_words[ind]:\n      lower_case_word = word.lower()\n      if lower_case_word not in stopwords:\n        tuple_list.append((ind, lower_case_word))\n  return tuple_list\n\nparse_row_udf = udf(parse_row, ArrayType(StructType([StructField('_1', LongType()),\n                                                         StructField('_2', StringType())])))\ntuple_electronics_meta_df = electronics_meta_df.select(electronics_meta_df.asin, parse_row_udf(electronics_meta_df.description, electronics_meta_df.title, electronics_meta_df.categories).alias('features'))\nelectronics_meta_df.unpersist()\ntuple_electronics_meta_df.cache().show(3)\n"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["a =  tuple_electronics_meta_df.select('features').first()[0]\n# parse_point(raw_df.select('text').first()[0])\nprint a"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":["from collections import defaultdict\nimport hashlib\n\ndef hash_function(raw_feats, num_buckets, print_mapping=False):\n    \"\"\"Calculate a feature dictionary for an observation's features based on hashing.\n\n    Note:\n        Use print_mapping=True for debug purposes and to better understand how the hashing works.\n\n    Args:\n        raw_feats (list of (int, str)): A list of features for an observation.  Represented as\n            (featureID, value) tuples.\n        num_buckets (int): Number of buckets to use as features.\n        print_mapping (bool, optional): If true, the mappings of featureString to index will be\n            printed.\n\n    Returns:\n        dict of int to float:  The keys will be integers which represent the buckets that the\n            features have been hashed to.  The value for a given key will contain the count of the\n            (featureID, value) tuples that have hashed to that key.\n    \"\"\"\n    mapping = { category + ':' + str(ind):\n                int(int(hashlib.md5(category + ':' + str(ind)).hexdigest(), 16) % num_buckets)\n                for ind, category in raw_feats}\n    if(print_mapping): print mapping\n\n    def map_update(l, r):\n        l[r] += 1.0\n        return l\n\n    sparse_features = reduce(map_update, mapping.values(), defaultdict(float))\n    return dict(sparse_features)"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":["hash_function(a, 10, True)"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"code","source":["from pyspark.ml.linalg import Vectors, VectorUDT\n\n\nnum_hash_buckets = 2 ** 3\n\n# UDF that returns a vector of hashed features given an Array of tuples\ntuples_to_hash_features_udf = udf(lambda x: Vectors.sparse(num_hash_buckets, hash_function(x, num_hash_buckets)), VectorUDT())\n\ndef add_hashed_features(df):\n    \"\"\"Return a DataFrame with labels and hashed features.\n\n    Note:\n        Make sure to cache the DataFrame that you are returning.\n\n    Args:\n        df (DataFrame with 'tuples' column): A DataFrame containing the tuples to be hashed.\n\n    Returns:\n        DataFrame: A DataFrame with a 'label' column and a 'features' column that contains a\n            SparseVector of hashed features.\n    \"\"\"\n    return (df.select(df.asin, tuples_to_hash_features_udf(df.features).alias('features')))\n\nhash_electronics_meta_df = add_hashed_features(tuple_electronics_meta_df)\ntuple_electronics_meta_df.unpersist()\nhash_electronics_meta_df.cache().show(3)"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"code","source":["print hash_electronics_meta_df.take(2)"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"code","source":["kmeans.explainParams()"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"code","source":["from pyspark.ml.clustering import KMeans\nfrom pyspark.ml.linalg import Vectors\n# data = [(Vectors.dense([0.0, 0.0]),), (Vectors.dense([1.0, 1.0]),),(Vectors.dense([9.0, 8.0]),), (Vectors.dense([8.0, 9.0]),)]\n# df = spark.createDataFrame(data, [\"features\"])\nkmeans = KMeans(k=2, seed=1, featuresCol=\"features\")\nmodel = kmeans.fit(hash_electronics_meta_df)\nmodel.computeCost(hash_electronics_meta_df)\ncenters = model.clusterCenters()\n\nprint centers"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"code","source":["transformed = model.transform(hash_electronics_meta_df).select(\"features\", \"prediction\")\nprint transformed.filter(transformed.prediction == 0).count()\nprint transformed.filter(transformed.prediction == 1).count()\n\nrows = transformed.collect()"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"code","source":["rows[:5]"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"code","source":["from pyspark.sql.functions import year, month, avg, count\nratings_by_year_df = ratings_df.select('overall', year('reviewTime').alias('Year')) \\\n                               .withColumnRenamed('overall', 'Rating') \\\n                               .groupby('Year').agg(avg('Rating').alias('Avg_rating'), count('Rating').alias('Num_reviews')) \\\n                               .orderBy('Year')\n\nratings_by_month_df = ratings_df.select('overall', month('reviewTime').alias('Month')) \\\n                               .withColumnRenamed('overall', 'Rating') \\\n                               .groupby('Month').agg(avg('Rating').alias('Avg_rating'), count('Rating').alias('Num_reviews')) \\\n                               .orderBy('Month')\n                               \nratings_by_year_df.show()\nratings_by_month_df.show()\n"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"code","source":["import matplotlib.pyplot as plt\nimport numpy as np\n# plt.scatter(ratings_by_year_df.Year.Values, ratings_by_year_df.Rating.Values)\nx = np.array(ratings_by_year_df.select('Year').collect())\ny = np.array(ratings_by_year_df.select('Avg_Rating').collect())\nfig, ax = plt.subplots()\nax.scatter(x,y)\nplt.title('Beauty_year')\ndisplay(fig)\n"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"code","source":["x = np.array(ratings_by_month_df.select('Month').collect())\ny = np.array(ratings_by_month_df.select('Avg_Rating').collect())\nfig2, ax2 = plt.subplots()\nax2.scatter(x,y)\nplt.title('Beauty_month')\ndisplay(fig2)"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"code","source":["ratings_df.describe('overall').show()"],"metadata":{},"outputs":[],"execution_count":22}],"metadata":{"name":"load_data","notebookId":105316977006135},"nbformat":4,"nbformat_minor":0}
