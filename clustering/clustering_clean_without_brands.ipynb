{"cells":[{"cell_type":"code","source":["# url_list = [('http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Musical_Instruments_5.json.gz', 'Musical_Instruments'),\n#            ('http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Amazon_Instant_Video_5.json.gz', 'Instant_Videos')]\nreview_list = [('http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Books_5.json.gz','Books'),\n       ('http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Electronics_5.json.gz','Electronics'),\n       ('http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Movies_and_TV_5.json.gz','Movies_and_TV'),\n       ('http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_CDs_and_Vinyl_5.json.gz','CDs_and_Vinyl')]\n#metadata_list = [('http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/meta_Amazon_Instant_Video.json.gz', 'Instant_Video')]\nmetadata_list = [('http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/meta_Electronics.json.gz', 'Electronics')]\n\nstop_words_url = 'http://tacit.usc.edu/resources/stopwords_eng.txt'"],"metadata":{},"outputs":[],"execution_count":1},{"cell_type":"code","source":["import urllib\nload_data = False\nload_metadata = False\nload_stop_words = False\n\nif load_data:\n  for url_tuple in review_list:\n    url = url_tuple[0]\n    folder_path = '/tmp/' + url_tuple[1] + '.json.gz'\n    urllib.urlretrieve(url, folder_path)\n    dbutils.fs.mv('file:' + folder_path, 'dbfs:/tmp/Data/' + url_tuple[1] + '.json.gz')\n    display(dbutils.fs.ls(\"dbfs:/tmp/Data\"))\n    \nif load_metadata:\n  for url_tuple in metadata_list:\n    url = url_tuple[0]\n    folder_path = '/tmp/' + url_tuple[1] + '.json.gz'\n    urllib.urlretrieve(url, folder_path)\n    dbutils.fs.mv('file:' + folder_path, 'dbfs:/tmp/Metadata/' + url_tuple[1] + '.json.gz')\n    display(dbutils.fs.ls(\"dbfs:/tmp/Metadata\"))\n    \nif load_stop_words:\n  urllib.urlretrieve(stop_words_url, '/tmp/stopwords_eng.txt')\n  dbutils.fs.mv('file:/tmp/stopwords_eng.txt', 'dbfs:/tmp/stopwords_eng.txt')\n  #   dbutils.fs.mv(\"file:/tmp/test.json.gz\", \"dbfs:/tmp/Data/test.json.gz\")\n  display(dbutils.fs.ls(\"dbfs:/tmp\"))"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["from pyspark.sql.types import *\n\nratings_df_schema = StructType(\n  [StructField('asin', StringType()),\n   StructField('helpful', ArrayType(IntegerType())),\n   StructField('overall', FloatType()),\n   StructField('reviewText', StringType()),\n   StructField('reviewTime', DateType()),\n   StructField('reviewerID', StringType()),\n   StructField('summary', StringType())]\n)\n# StructField('unixReviewTime', LongType())\n# ['asin', 'description', 'title', 'categories']\nmetadata_df_schema = StructType(\n  [StructField('asin', StringType()),\n   StructField('description', StringType()),\n   StructField('title', StringType()),\n   StructField('categories', ArrayType(StringType()))]\n)\n"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["#import 5-core review data\n\nimport_review_file = False\n\nif import_review_file:\n  import gzip\n  from pyspark.sql import Row\n  from collections import OrderedDict\n  from datetime import datetime\n\n  def parse_review(path):\n    g = gzip.open(path, 'rb')\n    for l in g:\n      yield eval(l)\n\n  def create_review_df(path):\n    ratings = []\n    for review in parse_review(path):\n      ratings.append(review)\n\n    #check if all entries contains a value for each key\n  #   missing_keys = []\n  #   all_keys = ['reviewerID', 'asin', 'reviewerName', 'helpful', 'unixReviewTime', 'reviewText', 'overall', 'reviewTime', 'summary']\n  #   for i in range(len(ratings)):\n  #     for j in range(len(all_keys)):\n  #       if (all_keys[j] not in ratings[i].keys()):\n  #         missing_keys.append(all_keys[j])\n  #   print set(missing_keys)\n    #not all entries have 'reviewerName'\n\n    #remove revivewerName, unixreviewTime from all entries and convert reivewTime to datetime.date type\n    for i in range(len(ratings)):\n      ratings[i].pop('reviewerName', None)\n      ratings[i].pop('unixReviewTime', None)\n      ratings[i]['reviewTime'] = datetime.strptime(ratings[i]['reviewTime'], '%m %d, %Y')\n\n    return sqlContext.createDataFrame(ratings, schema=ratings_df_schema)\n\n  # books_df = create_review_df('/dbfs/tmp/Data/Books.json.gz')\n  electronics_df = create_review_df('/dbfs/tmp/Data/Electronics.json.gz')\n  # movies_df = create_review_df('/dbfs/tmp/Data/Movies_and_TV.json.gz')\n  # cds_df = create_review_df('/dbfs/tmp/Data/CDs_and_Vinyl.json.gz')\n  # instruments_df = create_review_df('/dbfs/tmp/Data/Musical_Instruments.json.gz')\n  # instant_video_df = create_review_df('/dbfs/tmp/Data/Instant_Videos.json.gz')\n\n\n  # print books_df.cache().count()\n  print electronics_df.cache().count()\n  # print movies_df.cache().count()\n  # print cds_df.cache().count()\n  # print instruments_df.cache().count()\n  # print instant_video_df.cache().count()\n\n  electronics_df.show(3, truncate=True)"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["a = 'one word'\nb = 'a dskf sdfds sdf dsf one word'\nprint b.replace(a, \"\")\n"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["\n#import metadata files\n\nimport gzip\nfrom pyspark.sql import Row\nfrom collections import OrderedDict\nfrom datetime import datetime\n\ndef parse_metadata(path):\n  g = gzip.open(path, 'rb')\n  for l in g:\n    yield eval(l)\n    \ndef create_metadata_df(path):\n  metadata = []\n  for entry in parse_metadata(path):\n    metadata.append(entry)\n\n  #find all keys\n  all_keys = []\n  for i in range(len(metadata)):\n    for key in metadata[i].keys():\n      if key not in all_keys:\n        all_keys.append(key)\n  print all_keys\n  #['asin', 'categories', 'price', 'related'] in instant videos\n  #['asin', 'imUrl', 'description', 'categories', 'title', 'related', 'price', 'salesRank', 'brand'] in electronics\n  \n  #check number of entries that contains each key\n  key_count_dict = {}\n  for key in all_keys:\n    key_count_dict[key] = 0\n  for i in range(len(metadata)):\n    for key in all_keys:\n      if key in metadata[i].keys():\n        key_count_dict[key] += 1\n  print key_count_dict  \n#  {'asin': 498196, 'description': 459470, 'title': 491194, 'price': 389693, 'imUrl': 498021, 'related': 366959, 'salesRank': 128706, 'brand': 142532, 'categories': 498196} for electronics\n  discard_missing_info_col = True\n  cols_to_keep = ['asin', 'description', 'title', 'categories']\n  filtered_metadata = []\n  for i in range(len(metadata)):\n    keep_col = True\n    if discard_missing_info_col:\n      #only retain items that contain info for all columns\n      cols = [x for x in cols_to_keep if x in metadata[i].keys()]\n      if len(cols) != len(cols_to_keep):\n        keep_col = False\n    if keep_col:\n      temp_dict = {}\n      for col in cols_to_keep: \n        if col not in metadata[i].keys():\n          temp_dict[col] = None\n        elif col == 'categories':#given as array of array so only keeep the inner array\n          temp_dict[col] = metadata[i]['categories'][0]\n        else:\n          temp_dict[col] = metadata[i][col]\n      filtered_metadata.append(temp_dict)\n\n  return sqlContext.createDataFrame(filtered_metadata, schema = metadata_df_schema)#schema=metadata_df_schema\n  \n\n# books_df = create_review_df('/dbfs/tmp/Metadata/Books.json.gz')\nelectronics_meta_df = create_metadata_df('/dbfs/tmp/Metadata/Electronics.json.gz')\nelectronics_meta_df.cache().count()\n#instant_video_meta_df = create_metadata_df('/dbfs/tmp/Metadata/Instant_Video.json.gz')\n# movies_df = create_review_df('/dbfs/tmp/Metadata/Movies_and_TV.json.gz')\n# cds_df = create_review_df('/dbfs/tmp/Metadata/CDs_and_Vinyl.json.gz')\n# instruments_df = create_review_df('/dbfs/tmp/Metadata/Musical_Instruments.json.gz')\n# instant_video_df = create_review_df('/dbfs/tmp/Metadata/Instant_Videos.json.gz')\n\n\nelectronics_meta_df.show(20)\n\n"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["def find_brands(path):\n  brands = []\n  count = 0\n  for entry in parse_metadata(path):\n    if 'brand' in entry.keys() and entry['brand'] not in brands:\n      brands.append(entry['brand'])\n  return brands\n   \nbrands_stop_words = find_brands('/dbfs/tmp/Metadata/Electronics.json.gz')\nprint len(brands_stop_words)\n"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["print brands_stop_words[:12]"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["print electronics_meta_df.count()\n#489187 when including all entries\n#453246 when discarding entries with null elements in either category, descripton and title (discard_missing_info_col = True)"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["#print # of distinct categories among all products\nprint electronics_meta_df.select('categories').distinct().count()"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":["#import stopwords\nstopwords_path = 'dbfs:/tmp/stopwords_eng.txt'\nstopwords_rdd = sc.textFile(stopwords_path)\nstopwords = stopwords_rdd.collect()\nprint len(stopwords)\nprint stopwords\n"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":["#parse each column by itself instead\n# feature hashing\n#convert each row of metadata_df to list of tuple format \nfrom pyspark.sql.functions import udf, array, split\nimport re\n\ndef parse_row(col):\n  if col == None:\n    return [(0, 'EmptyString')]\n  elif isinstance(col, basestring):\n    #remove brands first\n    for brand in brands_stop_words:\n      if brand in col:\n        col = col.replace(brand, \"\")\n        break\n    words = re.split(r'\\W+',col)\n  elif type(col) is list:\n    words = col\n   \n  #assign featureID to its index in all_words (e.g. des_words_ID = 0, title_words_ID = 1, etc.)\n  tuple_list = []\n  for word in words:\n    lower_case_word = word.lower()\n    if (lower_case_word not in stopwords):\n      tuple_list.append((0, lower_case_word))\n  return tuple_list\n\n\nif not electronics_meta_df.is_cached:\n  electronics_meta_df.cache().count()\n\n  \nparse_row_udf = udf(parse_row, ArrayType(StructType([StructField('_1', LongType()),\n                                                         StructField('_2', StringType())])))\ntuple_electronics_meta_df = electronics_meta_df.select(electronics_meta_df.asin, \\\n                                                       parse_row_udf(electronics_meta_df.description).alias('description_features'), \\\n                                                       parse_row_udf(electronics_meta_df.title).alias('title_features'), \\\n                                                       parse_row_udf(electronics_meta_df.categories).alias('category_features'))\n                                                                                                                    \nif electronics_meta_df.is_cached:\n  electronics_meta_df.unpersist()\nif not tuple_electronics_meta_df.is_cached:\n  tuple_electronics_meta_df.cache().show(3)\n"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"code","source":["#check for null elements for each feature\nprint tuple_electronics_meta_df.where(tuple_electronics_meta_df.description_features.isNull()).count()\nprint tuple_electronics_meta_df.where(tuple_electronics_meta_df.title_features.isNull()).count()\nprint tuple_electronics_meta_df.where(tuple_electronics_meta_df.category_features.isNull()).count()\n"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"code","source":["from collections import defaultdict\nimport hashlib\n\ndef hash_function(raw_feats, num_buckets, print_mapping=False):\n    \"\"\"Calculate a feature dictionary for an observation's features based on hashing.\n\n    Note:\n        Use print_mapping=True for debug purposes and to better understand how the hashing works.\n\n    Args:\n        raw_feats (list of (int, str)): A list of features for an observation.  Represented as\n            (featureID, value) tuples.\n        num_buckets (int): Number of buckets to use as features.\n        print_mapping (bool, optional): If true, the mappings of featureString to index will be\n            printed.\n\n    Returns:\n        dict of int to float:  The keys will be integers which represent the buckets that the\n            features have been hashed to.  The value for a given key will contain the count of the\n            (featureID, value) tuples that have hashed to that key.\n    \"\"\"\n    mapping = { category + ':' + str(ind):\n                int(int(hashlib.md5(category + ':' + str(ind)).hexdigest(), 16) % num_buckets)\n                for ind, category in raw_feats}\n    if(print_mapping): print mapping\n\n    def map_update(l, r):\n        l[r] += 1.0\n        return l\n\n    sparse_features = reduce(map_update, mapping.values(), defaultdict(float))\n    return dict(sparse_features)"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"code","source":["from pyspark.ml.linalg import Vectors, VectorUDT\nimport numpy as np\n\n\nnum_hash_buckets_description = 2 ** 8\nnum_hash_buckets_title = 2 ** 9\nnum_hash_buckets_category = 2 ** 9\n\n# UDF that returns a vector of hashed features given an Array of tuples\n\ndef tuples_to_hash_features_desc(col):\n  return Vectors.sparse(num_hash_buckets_description, hash_function(col, num_hash_buckets_description))\ntuples_to_hash_features_desc_udf = udf(tuples_to_hash_features_desc, VectorUDT())\n\ndef tuples_to_hash_features_title(col):\n  return Vectors.sparse(num_hash_buckets_title, hash_function(col, num_hash_buckets_title))\ntuples_to_hash_features_title_udf = udf(tuples_to_hash_features_title, VectorUDT())\n\ndef tuples_to_hash_features_category(col):\n  return Vectors.sparse(num_hash_buckets_category, hash_function(col, num_hash_buckets_category))\ntuples_to_hash_features_category_udf = udf(tuples_to_hash_features_category, VectorUDT())\n\n\ndef add_hashed_features(df):\n    \"\"\"Return a DataFrame with labels and hashed features.\n\n    Note:\n        Make sure to cache the DataFrame that you are returning.\n\n    Args:\n        df (DataFrame with 'tuples' column): A DataFrame containing the tuples to be hashed.\n\n    Returns:\n        DataFrame: A DataFrame with a 'label' column and a 'features' column that contains a\n            SparseVector of hashed features.\n    \"\"\"\n    return (df.select(df.asin, \\\n                      tuples_to_hash_features_desc_udf(df.description_features).alias('description_hash_features'), \\\n                      tuples_to_hash_features_title_udf(df.title_features).alias('title_hash_features'), \\\n                      tuples_to_hash_features_category_udf(df.category_features).alias('category_hash_features')))\n\nhash_electronics_meta_df = add_hashed_features(tuple_electronics_meta_df)\n\ntuple_electronics_meta_df.unpersist()\nhash_electronics_meta_df.cache().show(3)"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"code","source":["print hash_electronics_meta_df.where(hash_electronics_meta_df.description_hash_features.isNull()).count()\nprint hash_electronics_meta_df.where(hash_electronics_meta_df.title_hash_features.isNull()).count()\nprint hash_electronics_meta_df.where(hash_electronics_meta_df.category_hash_features.isNull()).count()"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"code","source":["from pyspark.ml.linalg import Vectors, VectorUDT, SparseVector\n\n\n# combine_hash_features_udf = udf(lambda x1:x1, VectorUDT())\ndef combine_hash_features(col1, col2, col3):\n#   if isinstance(col1,SparseVector) and isinstance(col2,SparseVector) and isinstance(col3,SparseVector):\n  combined_array = np.concatenate((col1.toArray(),col2.toArray(), col3.toArray()))\n  sparse_vec = {i:combined_array[i] for i in np.nonzero(combined_array)[0]}\n  return Vectors.sparse(len(combined_array), sparse_vec)\n\ncombine_hash_features_udf = udf(combine_hash_features, VectorUDT())\n\nhash_combined_electronics_meta_df = hash_electronics_meta_df.select(hash_electronics_meta_df.asin, \\\n                                                                    combine_hash_features_udf(hash_electronics_meta_df.description_hash_features, \\\n                                                                                              hash_electronics_meta_df.title_hash_features, \\\n                                                                                              hash_electronics_meta_df.category_hash_features).alias('features'))\nhash_electronics_meta_df.unpersist()\nhash_combined_electronics_meta_df.cache().show(3)\n"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"code","source":["from pyspark.ml.clustering import KMeans\nfrom pyspark.ml.linalg import Vectors\n\nnum_clusters_list = [50, 100, 250, 500, 1000, 2000]\nscores = []\nmodels = []\nfor num_clusters in num_clusters_list:\n  kmeans = KMeans(k=num_clusters, seed=1, featuresCol=\"features\")\n  model = kmeans.fit(hash_combined_electronics_meta_df)\n  models.append(model)\n  score = model.computeCost(hash_combined_electronics_meta_df)\n  scores.append((num_clusters, score))\n\nprint scores\n#256, 512, 512\n# [(50, 30603181.219049413), (100, 29854189.077426285), (250, 28420735.469728597), (500, 27040367.94809037), (1000, 25572329.794782765), (2000, 23893421.253901474)]"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"code","source":["print model.summary.k"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"code","source":["\nimport plotly.offline as pyo\nimport plotly.graph_objects as go\n"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"code","source":["import plotly.offline as pyo\nimport plotly.graph_objs as go\nimport matplotlib.pyplot as plt\n\n\nx_val = [x[0] for x in scores]\ny_val = [x[1] for x in scores]\n\ntrace = go.Scatter(\n  x = x_val,\n  y = y_val\n)\n\nfigure = go.Figure(data=go.Data([trace]), layout=go.Layout())\ndisplayHTML(pyo.plot(figure, output_type='div'))"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"code","source":["for model in models:\n  filename = 'Kmeans_' + str(num_hash_buckets_description) + '_' + str(num_hash_buckets_title) + '_' + str(num_hash_buckets_category) + '_' + str(model.summary.k)\n  model.save(\"dbfs:/tmp/Clustering_results/\" + filename)\n"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"code","source":["dbutils.fs.ls(\"dbfs:/tmp/Clustering_results\")"],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"code","source":["from pyspark.ml.clustering import KMeans, KMeansModel\nbest_model = KMeansModel.load(\"dbfs:/tmp/Clustering_results/Kmeans_256_512_512_100/\")\ntransformed = best_model.transform(hash_combined_electronics_meta_df)\ncluster_size = []\n#for k in range(best_model.summary.k):\n # cluster_size.append((k, transformed.filter(transformed.prediction == k).count()))\n\n# rows = transformed.collect()"],"metadata":{},"outputs":[],"execution_count":24},{"cell_type":"code","source":["transformed.show(5)"],"metadata":{},"outputs":[],"execution_count":25},{"cell_type":"code","source":["rows[:5]"],"metadata":{},"outputs":[],"execution_count":26}],"metadata":{"name":"clustering_clean","notebookId":380649835595690},"nbformat":4,"nbformat_minor":0}
